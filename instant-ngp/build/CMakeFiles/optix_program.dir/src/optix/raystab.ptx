//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-27506705
// Cuda compilation tools, release 10.2, V10.2.89
// Based on LLVM 3.4svn
//

.version 6.5
.target sm_30
.address_size 64

	// .globl	__raygen__rg
.const .align 8 .b8 params[24];
.global .align 4 .f32 _ZZN4tcnn19gaussian_cdf_approxEffE20MAGIC_SIGMOID_FACTOR = 0f3F4ABDDD;
.global .align 4 .f32 _ZZN4tcnn30gaussian_cdf_approx_derivativeEffE20MAGIC_SIGMOID_FACTOR = 0f3F4ABDDD;
.global .align 4 .b8 _ZZN3ngp5sobolEjjE10directions[640] = {0, 0, 0, 128, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 128, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 128, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 128, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 160, 0, 0, 0, 240, 0, 0, 0, 136, 0, 0, 0, 204, 0, 0, 0, 170, 0, 0, 0, 255, 0, 0, 128, 128, 0, 0, 192, 192, 0, 0, 160, 160, 0, 0, 240, 240, 0, 0, 136, 136, 0, 0, 204, 204, 0, 0, 170, 170, 0, 0, 255, 255, 0, 128, 0, 128, 0, 192, 0, 192, 0, 160, 0, 160, 0, 240, 0, 240, 0, 136, 0, 136, 0, 204, 0, 204, 0, 170, 0, 170, 0, 255, 0, 255, 128, 128, 128, 128, 192, 192, 192, 192, 160, 160, 160, 160, 240, 240, 240, 240, 136, 136, 136, 136, 204, 204, 204, 204, 170, 170, 170, 170, 255, 255, 255, 255, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 96, 0, 0, 0, 144, 0, 0, 0, 232, 0, 0, 0, 92, 0, 0, 0, 142, 0, 0, 0, 197, 0, 0, 128, 104, 0, 0, 192, 156, 0, 0, 96, 238, 0, 0, 144, 85, 0, 0, 104, 128, 0, 0, 156, 192, 0, 0, 238, 96, 0, 0, 85, 144, 0, 128, 128, 232, 0, 192, 192, 92, 0, 96, 96, 142, 0, 144, 144, 197, 0, 232, 104, 104, 0, 92, 156, 156, 0, 142, 238, 238, 0, 197, 85, 85, 128, 232, 0, 128, 192, 92, 0, 192, 96, 142, 0, 96, 144, 197, 0, 144, 104, 104, 0, 232, 156, 156, 0, 92, 238, 238, 0, 142, 85, 85, 0, 197, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 32, 0, 0, 0, 80, 0, 0, 0, 248, 0, 0, 0, 116, 0, 0, 0, 162, 0, 0, 0, 147, 0, 0, 128, 216, 0, 0, 64, 37, 0, 0, 224, 89, 0, 0, 208, 230, 0, 0, 8, 120, 0, 0, 12, 180, 0, 0, 2, 130, 0, 0, 5, 195, 0, 128, 143, 32, 0, 64, 71, 81, 0, 32, 234, 251, 0, 48, 217, 117, 0, 136, 133, 160, 0, 84, 78, 145, 0, 158, 231, 219, 0, 109, 219, 37, 128, 0, 128, 88, 192, 0, 64, 229, 32, 0, 224, 121, 80, 0, 208, 182, 248, 0, 8, 128, 116, 0, 12, 192, 162, 0, 2, 32, 147, 0, 5, 80, 0, 0, 0, 128, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 176, 0, 0, 0, 248, 0, 0, 0, 220, 0, 0, 0, 122, 0, 0, 0, 157, 0, 0, 128, 90, 0, 0, 192, 47, 0, 0, 96, 161, 0, 0, 176, 240, 0, 0, 136, 218, 0, 0, 196, 111, 0, 0, 98, 129, 0, 0, 187, 64, 0, 128, 135, 34, 0, 192, 201, 179, 0, 160, 101, 251, 0, 208, 178, 221, 0, 40, 2, 120, 0, 60, 11, 156, 0, 182, 15, 90, 0, 219, 13, 45, 128, 128, 135, 162, 64, 192, 201, 243, 32, 160, 101, 219, 176, 208, 178, 109, 248, 40, 2, 128, 220, 60, 11, 64, 122, 182, 15, 32, 157, 219, 13, 176};
.global .align 4 .u32 _ZZ12__raygen__rgE11N_STAB_RAYS = 32;
.global .align 4 .f32 _ZZN3ngp13fibonacci_dirILj32EEEN5Eigen6MatrixIfLi3ELi1ELi0ELi3ELi1EEEjRKNS2_IfLi2ELi1ELi0ELi2ELi1EEEE12GOLDEN_RATIO = 0f3FCF1BBD;
.const .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .entry __raygen__rg(

)
{
	.local .align 4 .b8 	__local_depot0[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<19>;
	.reg .f32 	%f<80>;
	.reg .b32 	%r<164>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	// inline asm
	call (%r32), _optix_get_launch_index_x, ();
	// inline asm
	ld.const.u64 	%rd25, [params];
	cvta.to.global.u64 	%rd26, %rd25;
	cvt.u64.u32	%rd1, %r32;
	mul.wide.u32 	%rd27, %r32, 12;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.f32 	%f1, [%rd28];
	ld.global.f32 	%f2, [%rd28+4];
	ld.global.f32 	%f3, [%rd28+8];
	shl.b32 	%r1, %r32, 1;
	setp.eq.s32	%p1, %r1, 0;
	mov.u64 	%rd69, -8846114313915602277;
	mov.u64 	%rd68, 0;
	@%p1 bra 	BB0_4;

	cvt.u64.u32	%rd63, %r1;
	mov.u64 	%rd67, 1;
	mov.u64 	%rd66, -2720673578348880933;
	mov.u64 	%rd65, 6364136223846793005;
	mov.u64 	%rd68, 0;

BB0_2:
	and.b64  	%rd33, %rd63, 1;
	setp.eq.b64	%p2, %rd33, 1;
	not.pred 	%p3, %p2;
	mul.lo.s64 	%rd34, %rd68, %rd65;
	add.s64 	%rd35, %rd34, %rd66;
	selp.b64	%rd36, 1, %rd65, %p3;
	mul.lo.s64 	%rd67, %rd36, %rd67;
	selp.b64	%rd68, %rd68, %rd35, %p3;
	add.s64 	%rd37, %rd65, 1;
	mul.lo.s64 	%rd66, %rd37, %rd66;
	mul.lo.s64 	%rd65, %rd65, %rd65;
	shr.u64 	%rd63, %rd63, 1;
	setp.ne.s64	%p4, %rd63, 0;
	@%p4 bra 	BB0_2;

	mul.lo.s64 	%rd69, %rd67, -8846114313915602277;

BB0_4:
	add.s64 	%rd38, %rd68, %rd69;
	mul.lo.s64 	%rd39, %rd38, 6364136223846793005;
	add.s64 	%rd40, %rd39, -2720673578348880933;
	shr.u64 	%rd41, %rd38, 18;
	xor.b64  	%rd42, %rd41, %rd38;
	shr.u64 	%rd43, %rd42, 27;
	cvt.u32.u64	%r36, %rd43;
	shr.u64 	%rd44, %rd38, 59;
	cvt.u32.u64	%r37, %rd44;
	shr.u32 	%r38, %r36, %r37;
	neg.s32 	%r39, %r37;
	mov.u32 	%r155, 0;
	and.b32  	%r40, %r39, 31;
	shl.b32 	%r41, %r36, %r40;
	or.b32  	%r42, %r38, %r41;
	shr.u32 	%r43, %r42, 9;
	or.b32  	%r44, %r43, 1065353216;
	mov.b32 	 %f17, %r44;
	add.f32 	%f4, %f17, 0fBF800000;
	shr.u64 	%rd45, %rd40, 18;
	xor.b64  	%rd46, %rd45, %rd40;
	shr.u64 	%rd47, %rd46, 27;
	cvt.u32.u64	%r45, %rd47;
	shr.u64 	%rd48, %rd40, 59;
	cvt.u32.u64	%r46, %rd48;
	shr.u32 	%r47, %r45, %r46;
	neg.s32 	%r48, %r46;
	and.b32  	%r49, %r48, 31;
	shl.b32 	%r50, %r45, %r49;
	or.b32  	%r51, %r47, %r50;
	shr.u32 	%r52, %r51, 9;
	or.b32  	%r53, %r52, 1065353216;
	mov.b32 	 %f18, %r53;
	add.f32 	%f5, %f18, 0fBF800000;
	ld.const.u64 	%rd16, [params+16];

BB0_5:
	cvt.rn.f32.u32	%f19, %r155;
	add.f32 	%f20, %f19, 0f3FAA3D71;
	div.rn.f32 	%f21, %f20, 0f4206A3D7;
	add.f32 	%f22, %f4, %f21;
	cvt.rmi.f32.f32	%f23, %f22;
	sub.f32 	%f24, %f22, %f23;
	div.rn.f32 	%f25, %f19, 0f3FCF1BBD;
	add.f32 	%f26, %f5, %f25;
	cvt.rmi.f32.f32	%f27, %f26;
	sub.f32 	%f28, %f26, %f27;
	fma.rn.f32 	%f9, %f24, 0fC0000000, 0f3F800000;
	add.f32 	%f29, %f28, 0fBF000000;
	mul.f32 	%f10, %f29, 0f40C90FDB;
	mul.f32 	%f30, %f9, %f9;
	mov.f32 	%f31, 0f3F800000;
	sub.f32 	%f32, %f31, %f30;
	mov.f32 	%f33, 0f00000000;
	max.f32 	%f34, %f32, %f33;
	sqrt.rn.f32 	%f11, %f34;
	mul.f32 	%f35, %f10, 0f3F22F983;
	cvt.rni.s32.f32	%r163, %f35;
	cvt.rn.f32.s32	%f36, %r163;
	mov.f32 	%f37, 0fBFC90FDA;
	fma.rn.f32 	%f38, %f36, %f37, %f10;
	mov.f32 	%f39, 0fB3A22168;
	fma.rn.f32 	%f40, %f36, %f39, %f38;
	mov.f32 	%f41, 0fA7C234C5;
	fma.rn.f32 	%f79, %f36, %f41, %f40;
	abs.f32 	%f13, %f10;
	setp.leu.f32	%p5, %f13, 0f47CE4780;
	@%p5 bra 	BB0_16;

	setp.eq.f32	%p6, %f13, 0f7F800000;
	@%p6 bra 	BB0_15;
	bra.uni 	BB0_7;

BB0_15:
	mul.rn.f32 	%f79, %f10, %f33;
	bra.uni 	BB0_16;

BB0_7:
	mov.b32 	 %r4, %f10;
	shr.u32 	%r5, %r4, 23;
	bfe.u32 	%r56, %r4, 23, 8;
	add.s32 	%r57, %r56, -128;
	shl.b32 	%r58, %r4, 8;
	or.b32  	%r6, %r58, -2147483648;
	shr.u32 	%r7, %r57, 5;
	add.u64 	%rd50, %SP, 0;
	add.u64 	%rd71, %SPL, 0;
	mov.u32 	%r157, 0;
	mov.u64 	%rd70, __cudart_i2opi_f;
	mov.u32 	%r156, -6;

BB0_8:
	.pragma "nounroll";
	ld.const.u32 	%r61, [%rd70];
	// inline asm
	{
	mad.lo.cc.u32   %r59, %r61, %r6, %r157;
	madc.hi.u32     %r157, %r61, %r6,  0;
	}
	// inline asm
	st.local.u32 	[%rd71], %r59;
	add.s64 	%rd71, %rd71, 4;
	add.s64 	%rd70, %rd70, 4;
	add.s32 	%r156, %r156, 1;
	setp.ne.s32	%p7, %r156, 0;
	@%p7 bra 	BB0_8;

	and.b32  	%r12, %r4, -2147483648;
	cvta.to.local.u64 	%rd52, %rd50;
	st.local.u32 	[%rd52+24], %r157;
	mov.u32 	%r64, 6;
	sub.s32 	%r65, %r64, %r7;
	mul.wide.s32 	%rd53, %r65, 4;
	add.s64 	%rd22, %rd52, %rd53;
	ld.local.u32 	%r159, [%rd22];
	ld.local.u32 	%r158, [%rd22+-4];
	and.b32  	%r15, %r5, 31;
	setp.eq.s32	%p8, %r15, 0;
	@%p8 bra 	BB0_11;

	mov.u32 	%r66, 32;
	sub.s32 	%r67, %r66, %r15;
	shr.u32 	%r68, %r158, %r67;
	shl.b32 	%r69, %r159, %r15;
	add.s32 	%r159, %r68, %r69;
	ld.local.u32 	%r70, [%rd22+-8];
	shr.u32 	%r71, %r70, %r67;
	shl.b32 	%r72, %r158, %r15;
	add.s32 	%r158, %r71, %r72;

BB0_11:
	shr.u32 	%r73, %r158, 30;
	shl.b32 	%r74, %r159, 2;
	add.s32 	%r161, %r74, %r73;
	shl.b32 	%r21, %r158, 2;
	shr.u32 	%r75, %r161, 31;
	shr.u32 	%r76, %r159, 30;
	add.s32 	%r22, %r75, %r76;
	setp.eq.s32	%p9, %r75, 0;
	@%p9 bra 	BB0_12;
	bra.uni 	BB0_13;

BB0_12:
	mov.u32 	%r160, %r21;
	mov.u32 	%r162, %r12;
	bra.uni 	BB0_14;

BB0_13:
	not.b32 	%r77, %r161;
	neg.s32 	%r160, %r21;
	setp.eq.s32	%p10, %r21, 0;
	selp.u32	%r78, 1, 0, %p10;
	add.s32 	%r161, %r78, %r77;
	xor.b32  	%r162, %r12, -2147483648;

BB0_14:
	cvt.u64.u32	%rd54, %r161;
	shl.b64 	%rd55, %rd54, 32;
	cvt.u64.u32	%rd56, %r160;
	or.b64  	%rd57, %rd55, %rd56;
	cvt.rn.f64.s64	%fd1, %rd57;
	mul.f64 	%fd2, %fd1, 0d3BF921FB54442D19;
	cvt.rn.f32.f64	%f42, %fd2;
	neg.f32 	%f43, %f42;
	setp.eq.s32	%p11, %r162, 0;
	selp.f32	%f79, %f42, %f43, %p11;
	setp.eq.s32	%p12, %r12, 0;
	neg.s32 	%r79, %r22;
	selp.b32	%r163, %r22, %r79, %p12;

BB0_16:
	mul.f32 	%f54, %f79, %f79;
	mov.f32 	%f55, 0fBAB607ED;
	mov.f32 	%f56, 0f37CBAC00;
	fma.rn.f32 	%f57, %f56, %f54, %f55;
	mov.f32 	%f58, 0f3D2AAABB;
	fma.rn.f32 	%f59, %f57, %f54, %f58;
	mov.f32 	%f60, 0fBEFFFFFF;
	fma.rn.f32 	%f61, %f59, %f54, %f60;
	fma.rn.f32 	%f63, %f61, %f54, %f31;
	fma.rn.f32 	%f64, %f54, %f79, %f33;
	mov.f32 	%f65, 0f3C0885E4;
	mov.f32 	%f66, 0fB94D4153;
	fma.rn.f32 	%f67, %f66, %f54, %f65;
	mov.f32 	%f68, 0fBE2AAAA8;
	fma.rn.f32 	%f69, %f67, %f54, %f68;
	fma.rn.f32 	%f70, %f69, %f64, %f79;
	mov.u32 	%r118, 1;
	and.b32  	%r151, %r163, 1;
	setp.eq.b32	%p13, %r151, 1;
	not.pred 	%p14, %p13;
	mov.u32 	%r150, 0;
	selp.f32	%f71, %f70, %f63, %p14;
	selp.f32	%f72, %f63, %f70, %p14;
	and.b32  	%r152, %r163, 2;
	setp.eq.s32	%p15, %r152, 0;
	neg.f32 	%f73, %f71;
	selp.f32	%f74, %f71, %f73, %p15;
	add.s32 	%r153, %r163, 1;
	and.b32  	%r154, %r153, 2;
	setp.eq.s32	%p16, %r154, 0;
	neg.f32 	%f75, %f72;
	selp.f32	%f76, %f72, %f75, %p16;
	mul.f32 	%f48, %f11, %f76;
	mul.f32 	%f49, %f11, %f74;
	mov.u32 	%r113, 255;
	mov.u32 	%r114, 5;
	mov.f32 	%f52, 0f5A0E1BCA;
	// inline asm
	call(%r80,%r81,%r82,%r83,%r84,%r85,%r86,%r87,%r88,%r89,%r90,%r91,%r92,%r93,%r94,%r95,%r96,%r97,%r98,%r99,%r100,%r101,%r102,%r103,%r104,%r105,%r106,%r107,%r108,%r109,%r110,%r111),_optix_trace_typed_32,(%r150,%rd16,%f1,%f2,%f3,%f48,%f49,%f9,%f33,%f52,%f33,%r113,%r114,%r150,%r118,%r150,%r118,%r119,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150,%r150);
	// inline asm
	setp.eq.s32	%p17, %r80, 0;
	add.s32 	%r155, %r155, 1;
	@%p17 bra 	BB0_19;

	setp.lt.u32	%p18, %r155, 32;
	@%p18 bra 	BB0_5;

	ld.const.u64 	%rd59, [params+8];
	cvta.to.global.u64 	%rd60, %rd59;
	shl.b64 	%rd61, %rd1, 2;
	add.s64 	%rd62, %rd60, %rd61;
	ld.global.f32 	%f77, [%rd62];
	neg.f32 	%f78, %f77;
	st.global.f32 	[%rd62], %f78;

BB0_19:
	ret;
}

	// .globl	__miss__ms
.visible .entry __miss__ms(

)
{
	.reg .b32 	%r<3>;


	mov.u32 	%r2, 0;
	// inline asm
	call _optix_set_payload, (%r2, %r2);
	// inline asm
	ret;
}

	// .globl	__closesthit__ch
.visible .entry __closesthit__ch(

)
{
	.reg .b32 	%r<3>;


	mov.u32 	%r1, 0;
	mov.u32 	%r2, 1;
	// inline asm
	call _optix_set_payload, (%r1, %r2);
	// inline asm
	ret;
}


