//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-27506705
// Cuda compilation tools, release 10.2, V10.2.89
// Based on LLVM 3.4svn
//

.version 6.5
.target sm_30
.address_size 64

	// .globl	__raygen__rg
.const .align 8 .b8 params[32];
.global .align 4 .f32 _ZZN4tcnn19gaussian_cdf_approxEffE20MAGIC_SIGMOID_FACTOR = 0f3F4ABDDD;
.global .align 4 .f32 _ZZN4tcnn30gaussian_cdf_approx_derivativeEffE20MAGIC_SIGMOID_FACTOR = 0f3F4ABDDD;
.global .align 4 .b8 _ZZN3ngp5sobolEjjE10directions[640] = {0, 0, 0, 128, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 128, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 128, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 128, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 160, 0, 0, 0, 240, 0, 0, 0, 136, 0, 0, 0, 204, 0, 0, 0, 170, 0, 0, 0, 255, 0, 0, 128, 128, 0, 0, 192, 192, 0, 0, 160, 160, 0, 0, 240, 240, 0, 0, 136, 136, 0, 0, 204, 204, 0, 0, 170, 170, 0, 0, 255, 255, 0, 128, 0, 128, 0, 192, 0, 192, 0, 160, 0, 160, 0, 240, 0, 240, 0, 136, 0, 136, 0, 204, 0, 204, 0, 170, 0, 170, 0, 255, 0, 255, 128, 128, 128, 128, 192, 192, 192, 192, 160, 160, 160, 160, 240, 240, 240, 240, 136, 136, 136, 136, 204, 204, 204, 204, 170, 170, 170, 170, 255, 255, 255, 255, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 96, 0, 0, 0, 144, 0, 0, 0, 232, 0, 0, 0, 92, 0, 0, 0, 142, 0, 0, 0, 197, 0, 0, 128, 104, 0, 0, 192, 156, 0, 0, 96, 238, 0, 0, 144, 85, 0, 0, 104, 128, 0, 0, 156, 192, 0, 0, 238, 96, 0, 0, 85, 144, 0, 128, 128, 232, 0, 192, 192, 92, 0, 96, 96, 142, 0, 144, 144, 197, 0, 232, 104, 104, 0, 92, 156, 156, 0, 142, 238, 238, 0, 197, 85, 85, 128, 232, 0, 128, 192, 92, 0, 192, 96, 142, 0, 96, 144, 197, 0, 144, 104, 104, 0, 232, 156, 156, 0, 92, 238, 238, 0, 142, 85, 85, 0, 197, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 32, 0, 0, 0, 80, 0, 0, 0, 248, 0, 0, 0, 116, 0, 0, 0, 162, 0, 0, 0, 147, 0, 0, 128, 216, 0, 0, 64, 37, 0, 0, 224, 89, 0, 0, 208, 230, 0, 0, 8, 120, 0, 0, 12, 180, 0, 0, 2, 130, 0, 0, 5, 195, 0, 128, 143, 32, 0, 64, 71, 81, 0, 32, 234, 251, 0, 48, 217, 117, 0, 136, 133, 160, 0, 84, 78, 145, 0, 158, 231, 219, 0, 109, 219, 37, 128, 0, 128, 88, 192, 0, 64, 229, 32, 0, 224, 121, 80, 0, 208, 182, 248, 0, 8, 128, 116, 0, 12, 192, 162, 0, 2, 32, 147, 0, 5, 80, 0, 0, 0, 128, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 176, 0, 0, 0, 248, 0, 0, 0, 220, 0, 0, 0, 122, 0, 0, 0, 157, 0, 0, 128, 90, 0, 0, 192, 47, 0, 0, 96, 161, 0, 0, 176, 240, 0, 0, 136, 218, 0, 0, 196, 111, 0, 0, 98, 129, 0, 0, 187, 64, 0, 128, 135, 34, 0, 192, 201, 179, 0, 160, 101, 251, 0, 208, 178, 221, 0, 40, 2, 120, 0, 60, 11, 156, 0, 182, 15, 90, 0, 219, 13, 45, 128, 128, 135, 162, 64, 192, 201, 243, 32, 160, 101, 219, 176, 208, 178, 109, 248, 40, 2, 128, 220, 60, 11, 64, 122, 182, 15, 32, 157, 219, 13, 176};
.global .align 4 .u32 _ZZ12__raygen__rgE7N_PATHS = 32;
.global .align 4 .u32 _ZZ12__raygen__rgE9N_BOUNCES = 4;
.const .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .entry __raygen__rg(

)
{
	.local .align 4 .b8 	__local_depot0[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<130>;
	.reg .f32 	%f<703>;
	.reg .b32 	%r<881>;
	.reg .f64 	%fd<15>;
	.reg .b64 	%rd<234>;


	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	// inline asm
	call (%r219), _optix_get_launch_index_x, ();
	// inline asm
	ld.const.u64 	%rd80, [params];
	cvta.to.global.u64 	%rd81, %rd80;
	cvt.u64.u32	%rd1, %r219;
	mul.wide.u32 	%rd82, %r219, 12;
	add.s64 	%rd83, %rd81, %rd82;
	ld.global.f32 	%f1, [%rd83];
	ld.global.f32 	%f2, [%rd83+4];
	ld.global.f32 	%f3, [%rd83+8];
	shl.b32 	%r1, %r219, 9;
	setp.eq.s32	%p5, %r1, 0;
	mov.u64 	%rd212, 0;
	mov.u64 	%rd211, -8846114313915602277;
	@%p5 bra 	BB0_4;

	cvt.u64.u32	%rd210, %r1;
	mov.u64 	%rd212, 0;
	mov.u64 	%rd208, 1;
	mov.u64 	%rd207, -2720673578348880933;
	mov.u64 	%rd206, 6364136223846793005;

BB0_2:
	and.b64  	%rd88, %rd210, 1;
	setp.eq.b64	%p6, %rd88, 1;
	not.pred 	%p7, %p6;
	mul.lo.s64 	%rd89, %rd206, %rd212;
	add.s64 	%rd90, %rd89, %rd207;
	selp.b64	%rd212, %rd212, %rd90, %p7;
	selp.b64	%rd91, 1, %rd206, %p7;
	mul.lo.s64 	%rd208, %rd91, %rd208;
	add.s64 	%rd92, %rd206, 1;
	mul.lo.s64 	%rd207, %rd92, %rd207;
	mul.lo.s64 	%rd206, %rd206, %rd206;
	shr.u64 	%rd210, %rd210, 1;
	setp.ne.s64	%p8, %rd210, 0;
	@%p8 bra 	BB0_2;

	mul.lo.s64 	%rd211, %rd208, -8846114313915602277;

BB0_4:
	add.s64 	%rd232, %rd211, %rd212;
	ld.const.u64 	%rd17, [params+24];
	ld.const.u64 	%rd93, [params+8];
	cvta.to.global.u64 	%rd18, %rd93;
	add.u64 	%rd94, %SP, 0;
	add.u64 	%rd19, %SPL, 0;
	mov.u32 	%r816, 0;
	mov.u32 	%r817, %r816;
	bra.uni 	BB0_5;

BB0_122:
	mul.lo.s64 	%rd190, %rd232, 6364136223846793005;
	add.s64 	%rd65, %rd190, -2720673578348880933;
	shr.u64 	%rd191, %rd65, 18;
	xor.b64  	%rd192, %rd191, %rd65;
	shr.u64 	%rd193, %rd192, 27;
	cvt.u32.u64	%r788, %rd193;
	shr.u64 	%rd194, %rd65, 59;
	cvt.u32.u64	%r789, %rd194;
	shr.u32 	%r790, %r788, %r789;
	neg.s32 	%r791, %r789;
	and.b32  	%r792, %r791, 31;
	shl.b32 	%r793, %r788, %r792;
	or.b32  	%r794, %r790, %r793;
	shr.u32 	%r795, %r794, 9;
	or.b32  	%r796, %r795, 1065353216;
	mov.b32 	 %f655, %r796;
	add.f32 	%f656, %f655, 0fBF800000;
	mul.f32 	%f179, %f656, 0f40C90FDB;
	abs.f32 	%f657, %f179;
	setp.leu.f32	%p124, %f657, 0f47CE4780;
	setp.eq.f32	%p125, %f657, 0f7F800000;
	or.pred  	%p4, %p124, %p125;
	@%p4 bra 	BB0_129;

	mov.b32 	 %r799, %f179;
	shl.b32 	%r800, %r799, 8;
	or.b32  	%r205, %r800, -2147483648;
	mov.u64 	%rd228, __cudart_i2opi_f;
	mov.u32 	%r875, -6;
	mov.u64 	%rd229, %rd19;

BB0_124:
	.pragma "nounroll";
	ld.const.u32 	%r803, [%rd228];
	// inline asm
	{
	mad.lo.cc.u32   %r801, %r803, %r205, %r339;
	madc.hi.u32     %r339, %r803, %r205,  0;
	}
	// inline asm
	st.local.u32 	[%rd229], %r801;
	add.s64 	%rd229, %rd229, 4;
	add.s64 	%rd228, %rd228, 4;
	add.s32 	%r875, %r875, 1;
	setp.ne.s32	%p126, %r875, 0;
	@%p126 bra 	BB0_124;

	st.local.u32 	[%rd19+24], %r339;
	@%p4 bra 	BB0_129;

	mov.u32 	%r877, 0;
	mov.u64 	%rd231, 0;
	mov.u64 	%rd230, %rd19;
	mov.u32 	%r878, %r877;

BB0_127:
	.pragma "nounroll";
	shl.b64 	%rd197, %rd231, 2;
	mov.u64 	%rd198, __cudart_i2opi_f;
	add.s64 	%rd199, %rd198, %rd197;
	ld.const.u32 	%r812, [%rd199];
	// inline asm
	{
	mad.lo.cc.u32   %r810, %r812, %r205, %r878;
	madc.hi.u32     %r878, %r812, %r205,  0;
	}
	// inline asm
	st.local.u32 	[%rd230], %r810;
	add.s32 	%r877, %r877, 1;
	cvt.s64.s32	%rd231, %r877;
	mul.wide.s32 	%rd200, %r877, 4;
	add.s64 	%rd230, %rd19, %rd200;
	setp.ne.s32	%p127, %r877, 6;
	@%p127 bra 	BB0_127;

	st.local.u32 	[%rd19+24], %r878;

BB0_129:
	mul.lo.s64 	%rd201, %rd65, 6364136223846793005;
	add.s64 	%rd232, %rd201, -2720673578348880933;
	bra.uni 	BB0_131;

BB0_5:
	mul.lo.s64 	%rd95, %rd232, 6364136223846793005;
	add.s64 	%rd21, %rd95, -2720673578348880933;
	shr.u64 	%rd96, %rd232, 18;
	xor.b64  	%rd97, %rd96, %rd232;
	shr.u64 	%rd98, %rd97, 27;
	cvt.u32.u64	%r225, %rd98;
	shr.u64 	%rd99, %rd232, 59;
	cvt.u32.u64	%r226, %rd99;
	shr.u32 	%r227, %r225, %r226;
	neg.s32 	%r228, %r226;
	and.b32  	%r229, %r228, 31;
	shl.b32 	%r230, %r225, %r229;
	or.b32  	%r231, %r227, %r230;
	shr.u32 	%r232, %r231, 9;
	or.b32  	%r233, %r232, 1065353216;
	mov.b32 	 %f180, %r233;
	add.f32 	%f181, %f180, 0fBF800000;
	shr.u64 	%rd100, %rd21, 18;
	xor.b64  	%rd101, %rd100, %rd21;
	shr.u64 	%rd102, %rd101, 27;
	cvt.u32.u64	%r234, %rd102;
	shr.u64 	%rd103, %rd21, 59;
	cvt.u32.u64	%r235, %rd103;
	shr.u32 	%r236, %r234, %r235;
	neg.s32 	%r237, %r235;
	and.b32  	%r238, %r237, 31;
	shl.b32 	%r239, %r234, %r238;
	or.b32  	%r240, %r236, %r239;
	shr.u32 	%r241, %r240, 9;
	or.b32  	%r242, %r241, 1065353216;
	mov.b32 	 %f182, %r242;
	add.f32 	%f183, %f182, 0fBF800000;
	fma.rn.f32 	%f4, %f181, 0fC0000000, 0f3F800000;
	add.f32 	%f184, %f183, 0fBF000000;
	mul.f32 	%f5, %f184, 0f40C90FDB;
	mul.f32 	%f185, %f4, %f4;
	mov.f32 	%f186, 0f3F800000;
	sub.f32 	%f187, %f186, %f185;
	mov.f32 	%f188, 0f00000000;
	max.f32 	%f189, %f187, %f188;
	sqrt.rn.f32 	%f6, %f189;
	mul.f32 	%f190, %f5, 0f3F22F983;
	cvt.rni.s32.f32	%r826, %f190;
	cvt.rn.f32.s32	%f191, %r826;
	mov.f32 	%f192, 0fBFC90FDA;
	fma.rn.f32 	%f193, %f191, %f192, %f5;
	mov.f32 	%f194, 0fB3A22168;
	fma.rn.f32 	%f195, %f191, %f194, %f193;
	mov.f32 	%f196, 0fA7C234C5;
	fma.rn.f32 	%f666, %f191, %f196, %f195;
	abs.f32 	%f8, %f5;
	setp.leu.f32	%p9, %f8, 0f47CE4780;
	@%p9 bra 	BB0_16;

	setp.eq.f32	%p10, %f8, 0f7F800000;
	@%p10 bra 	BB0_15;
	bra.uni 	BB0_7;

BB0_15:
	mul.rn.f32 	%f666, %f5, %f188;
	bra.uni 	BB0_16;

BB0_7:
	mov.b32 	 %r6, %f5;
	shr.u32 	%r7, %r6, 23;
	bfe.u32 	%r245, %r6, 23, 8;
	add.s32 	%r246, %r245, -128;
	shl.b32 	%r247, %r6, 8;
	or.b32  	%r8, %r247, -2147483648;
	shr.u32 	%r9, %r246, 5;
	cvta.to.local.u64 	%rd215, %rd94;
	mov.u32 	%r820, 0;
	mov.u64 	%rd214, __cudart_i2opi_f;
	mov.u32 	%r819, -6;

BB0_8:
	.pragma "nounroll";
	ld.const.u32 	%r250, [%rd214];
	// inline asm
	{
	mad.lo.cc.u32   %r248, %r250, %r8, %r820;
	madc.hi.u32     %r820, %r250, %r8,  0;
	}
	// inline asm
	st.local.u32 	[%rd215], %r248;
	add.s64 	%rd215, %rd215, 4;
	add.s64 	%rd214, %rd214, 4;
	add.s32 	%r819, %r819, 1;
	setp.ne.s32	%p11, %r819, 0;
	@%p11 bra 	BB0_8;

	and.b32  	%r14, %r6, -2147483648;
	cvta.to.local.u64 	%rd107, %rd94;
	st.local.u32 	[%rd107+24], %r820;
	mov.u32 	%r253, 6;
	sub.s32 	%r254, %r253, %r9;
	mul.wide.s32 	%rd108, %r254, 4;
	add.s64 	%rd27, %rd107, %rd108;
	ld.local.u32 	%r822, [%rd27];
	ld.local.u32 	%r821, [%rd27+-4];
	and.b32  	%r17, %r7, 31;
	setp.eq.s32	%p12, %r17, 0;
	@%p12 bra 	BB0_11;

	mov.u32 	%r255, 32;
	sub.s32 	%r256, %r255, %r17;
	shr.u32 	%r257, %r821, %r256;
	shl.b32 	%r258, %r822, %r17;
	add.s32 	%r822, %r257, %r258;
	ld.local.u32 	%r259, [%rd27+-8];
	shr.u32 	%r260, %r259, %r256;
	shl.b32 	%r261, %r821, %r17;
	add.s32 	%r821, %r260, %r261;

BB0_11:
	shr.u32 	%r262, %r821, 30;
	shl.b32 	%r263, %r822, 2;
	add.s32 	%r824, %r263, %r262;
	shl.b32 	%r23, %r821, 2;
	shr.u32 	%r264, %r824, 31;
	shr.u32 	%r265, %r822, 30;
	add.s32 	%r24, %r264, %r265;
	setp.eq.s32	%p13, %r264, 0;
	@%p13 bra 	BB0_12;
	bra.uni 	BB0_13;

BB0_12:
	mov.u32 	%r823, %r23;
	mov.u32 	%r825, %r14;
	bra.uni 	BB0_14;

BB0_13:
	not.b32 	%r266, %r824;
	neg.s32 	%r823, %r23;
	setp.eq.s32	%p14, %r23, 0;
	selp.u32	%r267, 1, 0, %p14;
	add.s32 	%r824, %r267, %r266;
	xor.b32  	%r825, %r14, -2147483648;

BB0_14:
	cvt.u64.u32	%rd109, %r824;
	shl.b64 	%rd110, %rd109, 32;
	cvt.u64.u32	%rd111, %r823;
	or.b64  	%rd112, %rd110, %rd111;
	cvt.rn.f64.s64	%fd1, %rd112;
	mul.f64 	%fd2, %fd1, 0d3BF921FB54442D19;
	cvt.rn.f32.f64	%f197, %fd2;
	neg.f32 	%f198, %f197;
	setp.eq.s32	%p15, %r825, 0;
	selp.f32	%f666, %f197, %f198, %p15;
	setp.eq.s32	%p16, %r14, 0;
	neg.s32 	%r268, %r24;
	selp.b32	%r826, %r24, %r268, %p16;

BB0_16:
	mul.f32 	%f209, %f666, %f666;
	mov.f32 	%f210, 0fBAB607ED;
	mov.f32 	%f211, 0f37CBAC00;
	fma.rn.f32 	%f212, %f211, %f209, %f210;
	mov.f32 	%f213, 0f3D2AAABB;
	fma.rn.f32 	%f214, %f212, %f209, %f213;
	mov.f32 	%f215, 0fBEFFFFFF;
	fma.rn.f32 	%f216, %f214, %f209, %f215;
	fma.rn.f32 	%f218, %f216, %f209, %f186;
	fma.rn.f32 	%f219, %f209, %f666, %f188;
	mov.f32 	%f220, 0f3C0885E4;
	mov.f32 	%f671, 0fB94D4153;
	fma.rn.f32 	%f222, %f671, %f209, %f220;
	mov.f32 	%f223, 0fBE2AAAA8;
	fma.rn.f32 	%f224, %f222, %f209, %f223;
	fma.rn.f32 	%f225, %f224, %f219, %f666;
	mov.u32 	%r307, 1;
	and.b32  	%r340, %r826, 1;
	setp.eq.b32	%p17, %r340, 1;
	not.pred 	%p18, %p17;
	mov.u32 	%r339, 0;
	selp.f32	%f226, %f225, %f218, %p18;
	selp.f32	%f227, %f218, %f225, %p18;
	and.b32  	%r341, %r826, 2;
	setp.eq.s32	%p19, %r341, 0;
	neg.f32 	%f228, %f226;
	selp.f32	%f229, %f226, %f228, %p19;
	add.s32 	%r342, %r826, 1;
	and.b32  	%r343, %r342, 2;
	setp.eq.s32	%p20, %r343, 0;
	neg.f32 	%f230, %f227;
	selp.f32	%f231, %f227, %f230, %p20;
	mul.f32 	%f203, %f6, %f231;
	mul.f32 	%f204, %f6, %f229;
	mul.lo.s64 	%rd114, %rd21, 6364136223846793005;
	add.s64 	%rd232, %rd114, -2720673578348880933;
	mov.u32 	%r302, 255;
	mov.f32 	%f207, 0f5A0E1BCA;
	// inline asm
	call(%r269,%r270,%r271,%r272,%r273,%r274,%r275,%r276,%r277,%r278,%r279,%r280,%r281,%r282,%r283,%r284,%r285,%r286,%r287,%r288,%r289,%r290,%r291,%r292,%r293,%r294,%r295,%r296,%r297,%r298,%r299,%r300),_optix_trace_typed_32,(%r339,%rd17,%f1,%f2,%f3,%f203,%f204,%f4,%f188,%f207,%f188,%r302,%r307,%r339,%r307,%r339,%r307,%r879,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339);
	// inline asm
	setp.eq.s32	%p21, %r269, -1;
	@%p21 bra 	BB0_130;

	mul.wide.u32 	%rd115, %r269, 36;
	add.s64 	%rd116, %rd18, %rd115;
	ld.global.f32 	%f232, [%rd116];
	ld.global.f32 	%f233, [%rd116+12];
	sub.f32 	%f234, %f233, %f232;
	ld.global.f32 	%f235, [%rd116+4];
	ld.global.f32 	%f236, [%rd116+16];
	sub.f32 	%f237, %f236, %f235;
	ld.global.f32 	%f238, [%rd116+8];
	ld.global.f32 	%f239, [%rd116+20];
	sub.f32 	%f240, %f239, %f238;
	ld.global.f32 	%f241, [%rd116+24];
	sub.f32 	%f242, %f241, %f232;
	ld.global.f32 	%f243, [%rd116+28];
	sub.f32 	%f244, %f243, %f235;
	ld.global.f32 	%f245, [%rd116+32];
	sub.f32 	%f246, %f245, %f238;
	sub.f32 	%f247, %f1, %f232;
	sub.f32 	%f248, %f2, %f235;
	sub.f32 	%f249, %f3, %f238;
	mul.f32 	%f250, %f237, %f246;
	mul.f32 	%f251, %f240, %f244;
	sub.f32 	%f252, %f250, %f251;
	mul.f32 	%f253, %f240, %f242;
	mul.f32 	%f254, %f234, %f246;
	sub.f32 	%f255, %f253, %f254;
	mul.f32 	%f256, %f234, %f244;
	mul.f32 	%f257, %f237, %f242;
	sub.f32 	%f258, %f256, %f257;
	mul.f32 	%f259, %f4, %f248;
	mul.f32 	%f260, %f204, %f249;
	sub.f32 	%f261, %f259, %f260;
	mul.f32 	%f262, %f203, %f249;
	mul.f32 	%f263, %f4, %f247;
	sub.f32 	%f264, %f262, %f263;
	mul.f32 	%f265, %f204, %f247;
	mul.f32 	%f266, %f203, %f248;
	sub.f32 	%f267, %f265, %f266;
	mul.f32 	%f268, %f203, %f252;
	mul.f32 	%f269, %f204, %f255;
	mul.f32 	%f270, %f4, %f258;
	add.f32 	%f271, %f270, %f269;
	add.f32 	%f272, %f268, %f271;
	rcp.rn.f32 	%f273, %f272;
	mul.f32 	%f274, %f244, %f264;
	fma.rn.f32 	%f275, %f246, %f267, %f274;
	fma.rn.f32 	%f276, %f242, %f261, %f275;
	mul.f32 	%f277, %f273, %f276;
	mul.f32 	%f278, %f237, %f264;
	fma.rn.f32 	%f279, %f240, %f267, %f278;
	fma.rn.f32 	%f280, %f234, %f261, %f279;
	mul.f32 	%f281, %f273, %f280;
	mul.f32 	%f282, %f258, %f249;
	fma.rn.f32 	%f283, %f255, %f248, %f282;
	fma.rn.f32 	%f284, %f247, %f252, %f283;
	mul.f32 	%f14, %f273, %f284;
	setp.gt.f32	%p22, %f277, 0f80000000;
	setp.lt.f32	%p23, %f277, 0fBF800000;
	or.pred  	%p24, %p22, %p23;
	setp.lt.f32	%p25, %f281, 0f00000000;
	or.pred  	%p26, %p24, %p25;
	sub.f32 	%f285, %f281, %f277;
	setp.gt.f32	%p27, %f285, 0f3F800000;
	or.pred  	%p1, %p26, %p27;
	neg.f32 	%f286, %f270;
	sub.f32 	%f287, %f286, %f269;
	sub.f32 	%f288, %f287, %f268;
	mov.b32 	 %r344, %f288;
	and.b32  	%r345, %r344, -2147483648;
	or.b32  	%r346, %r345, 1065353216;
	mov.b32 	 %f289, %r346;
	mul.f32 	%f669, %f252, %f289;
	mul.f32 	%f668, %f255, %f289;
	mul.f32 	%f667, %f258, %f289;
	mul.f32 	%f290, %f667, %f667;
	fma.rn.f32 	%f291, %f668, %f668, %f290;
	fma.rn.f32 	%f18, %f669, %f669, %f291;
	setp.leu.f32	%p28, %f18, 0f00000000;
	@%p28 bra 	BB0_19;

	sqrt.rn.f32 	%f292, %f18;
	div.rn.f32 	%f669, %f669, %f292;
	div.rn.f32 	%f668, %f668, %f292;
	div.rn.f32 	%f667, %f667, %f292;

BB0_19:
	mov.f32 	%f293, 0fBA83126F;
	sub.f32 	%f294, %f293, %f14;
	setp.gt.f32	%p29, %f14, 0f80000000;
	or.pred  	%p30, %p1, %p29;
	selp.f32	%f295, 0f7F7FFFFF, %f294, %p30;
	max.f32 	%f297, %f188, %f295;
	fma.rn.f32 	%f25, %f203, %f297, %f1;
	fma.rn.f32 	%f26, %f204, %f297, %f2;
	fma.rn.f32 	%f27, %f4, %f297, %f3;
	mul.lo.s64 	%rd117, %rd232, 6364136223846793005;
	add.s64 	%rd29, %rd117, -2720673578348880933;
	shr.u64 	%rd118, %rd232, 18;
	xor.b64  	%rd119, %rd118, %rd232;
	shr.u64 	%rd120, %rd119, 27;
	cvt.u32.u64	%r347, %rd120;
	shr.u64 	%rd121, %rd232, 59;
	cvt.u32.u64	%r348, %rd121;
	shr.u32 	%r349, %r347, %r348;
	neg.s32 	%r350, %r348;
	and.b32  	%r351, %r350, 31;
	shl.b32 	%r352, %r347, %r351;
	or.b32  	%r353, %r349, %r352;
	shr.u32 	%r354, %r353, 9;
	or.b32  	%r355, %r354, 1065353216;
	mov.b32 	 %f298, %r355;
	add.f32 	%f299, %f298, 0fBF800000;
	shr.u64 	%rd122, %rd29, 18;
	xor.b64  	%rd123, %rd122, %rd29;
	shr.u64 	%rd124, %rd123, 27;
	cvt.u32.u64	%r356, %rd124;
	shr.u64 	%rd125, %rd29, 59;
	cvt.u32.u64	%r357, %rd125;
	shr.u32 	%r358, %r356, %r357;
	neg.s32 	%r359, %r357;
	and.b32  	%r360, %r359, 31;
	shl.b32 	%r361, %r356, %r360;
	or.b32  	%r362, %r358, %r361;
	shr.u32 	%r363, %r362, 9;
	or.b32  	%r364, %r363, 1065353216;
	mov.b32 	 %f300, %r364;
	add.f32 	%f301, %f300, 0fBF800000;
	sqrt.rn.f32 	%f28, %f299;
	mul.f32 	%f29, %f301, 0f40C90FDB;
	mul.f32 	%f302, %f29, 0f3F22F983;
	cvt.rni.s32.f32	%r842, %f302;
	cvt.rn.f32.s32	%f303, %r842;
	fma.rn.f32 	%f305, %f303, %f192, %f29;
	fma.rn.f32 	%f307, %f303, %f194, %f305;
	fma.rn.f32 	%f673, %f303, %f196, %f307;
	abs.f32 	%f31, %f29;
	setp.leu.f32	%p31, %f31, 0f47CE4780;
	mov.u32 	%r834, %r842;
	mov.f32 	%f670, %f673;
	@%p31 bra 	BB0_30;

	setp.eq.f32	%p32, %f31, 0f7F800000;
	@%p32 bra 	BB0_29;
	bra.uni 	BB0_21;

BB0_29:
	mul.rn.f32 	%f670, %f29, %f188;
	mov.u32 	%r834, %r842;
	bra.uni 	BB0_30;

BB0_21:
	mov.b32 	 %r35, %f29;
	shr.u32 	%r36, %r35, 23;
	shl.b32 	%r367, %r35, 8;
	or.b32  	%r37, %r367, -2147483648;
	mov.u32 	%r828, 0;
	mov.u64 	%rd216, __cudart_i2opi_f;
	mov.u32 	%r827, -6;
	mov.u64 	%rd217, %rd19;

BB0_22:
	.pragma "nounroll";
	ld.const.u32 	%r370, [%rd216];
	// inline asm
	{
	mad.lo.cc.u32   %r368, %r370, %r37, %r828;
	madc.hi.u32     %r828, %r370, %r37,  0;
	}
	// inline asm
	st.local.u32 	[%rd217], %r368;
	add.s64 	%rd217, %rd217, 4;
	add.s64 	%rd216, %rd216, 4;
	add.s32 	%r827, %r827, 1;
	setp.ne.s32	%p33, %r827, 0;
	@%p33 bra 	BB0_22;

	and.b32  	%r373, %r36, 255;
	add.s32 	%r374, %r373, -128;
	shr.u32 	%r375, %r374, 5;
	and.b32  	%r42, %r35, -2147483648;
	st.local.u32 	[%rd19+24], %r828;
	mov.u32 	%r376, 6;
	sub.s32 	%r377, %r376, %r375;
	mul.wide.s32 	%rd127, %r377, 4;
	add.s64 	%rd34, %rd19, %rd127;
	ld.local.u32 	%r830, [%rd34];
	ld.local.u32 	%r829, [%rd34+-4];
	and.b32  	%r45, %r36, 31;
	setp.eq.s32	%p34, %r45, 0;
	@%p34 bra 	BB0_25;

	mov.u32 	%r378, 32;
	sub.s32 	%r379, %r378, %r45;
	shr.u32 	%r380, %r829, %r379;
	shl.b32 	%r381, %r830, %r45;
	add.s32 	%r830, %r380, %r381;
	ld.local.u32 	%r382, [%rd34+-8];
	shr.u32 	%r383, %r382, %r379;
	shl.b32 	%r384, %r829, %r45;
	add.s32 	%r829, %r383, %r384;

BB0_25:
	shr.u32 	%r385, %r829, 30;
	shl.b32 	%r386, %r830, 2;
	add.s32 	%r832, %r386, %r385;
	shl.b32 	%r51, %r829, 2;
	shr.u32 	%r387, %r832, 31;
	shr.u32 	%r388, %r830, 30;
	add.s32 	%r52, %r387, %r388;
	setp.eq.s32	%p35, %r387, 0;
	@%p35 bra 	BB0_26;
	bra.uni 	BB0_27;

BB0_26:
	mov.u32 	%r831, %r51;
	mov.u32 	%r833, %r42;
	bra.uni 	BB0_28;

BB0_27:
	not.b32 	%r389, %r832;
	neg.s32 	%r831, %r51;
	setp.eq.s32	%p36, %r51, 0;
	selp.u32	%r390, 1, 0, %p36;
	add.s32 	%r832, %r390, %r389;
	xor.b32  	%r833, %r42, -2147483648;

BB0_28:
	cvt.u64.u32	%rd128, %r832;
	shl.b64 	%rd129, %rd128, 32;
	cvt.u64.u32	%rd130, %r831;
	or.b64  	%rd131, %rd129, %rd130;
	cvt.rn.f64.s64	%fd3, %rd131;
	mul.f64 	%fd4, %fd3, 0d3BF921FB54442D19;
	cvt.rn.f32.f64	%f309, %fd4;
	neg.f32 	%f310, %f309;
	setp.eq.s32	%p37, %r833, 0;
	selp.f32	%f670, %f309, %f310, %p37;
	setp.eq.s32	%p38, %r42, 0;
	neg.s32 	%r391, %r52;
	selp.b32	%r834, %r52, %r391, %p38;

BB0_30:
	add.s32 	%r61, %r834, 1;
	and.b32  	%r62, %r61, 1;
	setp.eq.s32	%p39, %r62, 0;
	selp.f32	%f35, %f670, 0f3F800000, %p39;
	mul.rn.f32 	%f36, %f670, %f670;
	fma.rn.f32 	%f37, %f36, %f35, %f188;
	@%p39 bra 	BB0_32;

	fma.rn.f32 	%f671, %f211, %f36, %f210;

BB0_32:
	selp.f32	%f316, 0f3C0885E4, 0f3D2AAABB, %p39;
	fma.rn.f32 	%f317, %f671, %f36, %f316;
	selp.f32	%f318, 0fBE2AAAA8, 0fBEFFFFFF, %p39;
	fma.rn.f32 	%f319, %f317, %f36, %f318;
	fma.rn.f32 	%f672, %f319, %f37, %f35;
	and.b32  	%r392, %r61, 2;
	setp.eq.s32	%p41, %r392, 0;
	@%p41 bra 	BB0_34;

	mov.f32 	%f321, 0fBF800000;
	fma.rn.f32 	%f672, %f672, %f321, %f188;

BB0_34:
	mul.f32 	%f43, %f28, %f672;
	@%p31 bra 	BB0_45;

	setp.eq.f32	%p43, %f31, 0f7F800000;
	@%p43 bra 	BB0_44;
	bra.uni 	BB0_36;

BB0_44:
	mul.rn.f32 	%f673, %f29, %f188;
	bra.uni 	BB0_45;

BB0_36:
	mov.b32 	 %r63, %f29;
	shr.u32 	%r64, %r63, 23;
	shl.b32 	%r395, %r63, 8;
	or.b32  	%r65, %r395, -2147483648;
	mov.u32 	%r836, 0;
	mov.u64 	%rd218, __cudart_i2opi_f;
	mov.u32 	%r835, -6;
	mov.u64 	%rd219, %rd19;

BB0_37:
	.pragma "nounroll";
	ld.const.u32 	%r398, [%rd218];
	// inline asm
	{
	mad.lo.cc.u32   %r396, %r398, %r65, %r836;
	madc.hi.u32     %r836, %r398, %r65,  0;
	}
	// inline asm
	st.local.u32 	[%rd219], %r396;
	add.s64 	%rd219, %rd219, 4;
	add.s64 	%rd218, %rd218, 4;
	add.s32 	%r835, %r835, 1;
	setp.ne.s32	%p44, %r835, 0;
	@%p44 bra 	BB0_37;

	and.b32  	%r401, %r64, 255;
	add.s32 	%r402, %r401, -128;
	shr.u32 	%r403, %r402, 5;
	and.b32  	%r70, %r63, -2147483648;
	st.local.u32 	[%rd19+24], %r836;
	mov.u32 	%r404, 6;
	sub.s32 	%r405, %r404, %r403;
	mul.wide.s32 	%rd133, %r405, 4;
	add.s64 	%rd39, %rd19, %rd133;
	ld.local.u32 	%r838, [%rd39];
	ld.local.u32 	%r837, [%rd39+-4];
	and.b32  	%r73, %r64, 31;
	setp.eq.s32	%p45, %r73, 0;
	@%p45 bra 	BB0_40;

	mov.u32 	%r406, 32;
	sub.s32 	%r407, %r406, %r73;
	shr.u32 	%r408, %r837, %r407;
	shl.b32 	%r409, %r838, %r73;
	add.s32 	%r838, %r408, %r409;
	ld.local.u32 	%r410, [%rd39+-8];
	shr.u32 	%r411, %r410, %r407;
	shl.b32 	%r412, %r837, %r73;
	add.s32 	%r837, %r411, %r412;

BB0_40:
	shr.u32 	%r413, %r837, 30;
	shl.b32 	%r414, %r838, 2;
	add.s32 	%r840, %r414, %r413;
	shl.b32 	%r79, %r837, 2;
	shr.u32 	%r415, %r840, 31;
	shr.u32 	%r416, %r838, 30;
	add.s32 	%r80, %r415, %r416;
	setp.eq.s32	%p46, %r415, 0;
	@%p46 bra 	BB0_41;
	bra.uni 	BB0_42;

BB0_41:
	mov.u32 	%r839, %r79;
	mov.u32 	%r841, %r70;
	bra.uni 	BB0_43;

BB0_42:
	not.b32 	%r417, %r840;
	neg.s32 	%r839, %r79;
	setp.eq.s32	%p47, %r79, 0;
	selp.u32	%r418, 1, 0, %p47;
	add.s32 	%r840, %r418, %r417;
	xor.b32  	%r841, %r70, -2147483648;

BB0_43:
	cvt.u64.u32	%rd134, %r840;
	shl.b64 	%rd135, %rd134, 32;
	cvt.u64.u32	%rd136, %r839;
	or.b64  	%rd137, %rd135, %rd136;
	cvt.rn.f64.s64	%fd5, %rd137;
	mul.f64 	%fd6, %fd5, 0d3BF921FB54442D19;
	cvt.rn.f32.f64	%f322, %fd6;
	neg.f32 	%f323, %f322;
	setp.eq.s32	%p48, %r841, 0;
	selp.f32	%f673, %f322, %f323, %p48;
	setp.eq.s32	%p49, %r70, 0;
	neg.s32 	%r419, %r80;
	selp.b32	%r842, %r80, %r419, %p49;

BB0_45:
	and.b32  	%r89, %r842, 1;
	setp.eq.s32	%p50, %r89, 0;
	selp.f32	%f47, %f673, 0f3F800000, %p50;
	mul.rn.f32 	%f48, %f673, %f673;
	fma.rn.f32 	%f49, %f48, %f47, %f188;
	mov.f32 	%f674, 0fB94D4153;
	@%p50 bra 	BB0_47;

	fma.rn.f32 	%f674, %f211, %f48, %f210;

BB0_47:
	selp.f32	%f329, 0f3C0885E4, 0f3D2AAABB, %p50;
	fma.rn.f32 	%f330, %f674, %f48, %f329;
	selp.f32	%f331, 0fBE2AAAA8, 0fBEFFFFFF, %p50;
	fma.rn.f32 	%f332, %f330, %f48, %f331;
	fma.rn.f32 	%f675, %f332, %f49, %f47;
	and.b32  	%r420, %r842, 2;
	setp.eq.s32	%p52, %r420, 0;
	@%p52 bra 	BB0_49;

	mov.f32 	%f334, 0fBF800000;
	fma.rn.f32 	%f675, %f675, %f334, %f188;

BB0_49:
	mul.f32 	%f335, %f43, %f43;
	sub.f32 	%f337, %f186, %f335;
	mul.f32 	%f55, %f28, %f675;
	mul.f32 	%f338, %f55, %f55;
	sub.f32 	%f339, %f337, %f338;
	max.f32 	%f341, %f188, %f339;
	sqrt.rn.f32 	%f56, %f341;
	abs.f32 	%f342, %f667;
	abs.f32 	%f343, %f669;
	setp.gt.f32	%p53, %f343, %f342;
	neg.f32 	%f344, %f668;
	selp.f32	%f678, %f344, 0f00000000, %p53;
	neg.f32 	%f345, %f667;
	selp.f32	%f677, %f669, %f345, %p53;
	selp.f32	%f676, 0f00000000, %f668, %p53;
	mul.f32 	%f346, %f676, %f676;
	fma.rn.f32 	%f347, %f677, %f677, %f346;
	fma.rn.f32 	%f60, %f678, %f678, %f347;
	setp.leu.f32	%p54, %f60, 0f00000000;
	@%p54 bra 	BB0_51;

	sqrt.rn.f32 	%f348, %f60;
	div.rn.f32 	%f678, %f678, %f348;
	div.rn.f32 	%f677, %f677, %f348;
	div.rn.f32 	%f676, %f676, %f348;

BB0_51:
	mul.f32 	%f358, %f668, %f676;
	mul.f32 	%f359, %f667, %f677;
	sub.f32 	%f360, %f359, %f358;
	mul.f32 	%f361, %f667, %f678;
	mul.f32 	%f362, %f669, %f676;
	sub.f32 	%f363, %f362, %f361;
	mul.f32 	%f364, %f669, %f677;
	mul.f32 	%f365, %f668, %f678;
	sub.f32 	%f366, %f365, %f364;
	mul.f32 	%f367, %f43, %f360;
	fma.rn.f32 	%f368, %f55, %f678, %f367;
	fma.rn.f32 	%f67, %f669, %f56, %f368;
	mul.f32 	%f369, %f43, %f363;
	fma.rn.f32 	%f370, %f55, %f677, %f369;
	fma.rn.f32 	%f68, %f668, %f56, %f370;
	mul.f32 	%f371, %f43, %f366;
	fma.rn.f32 	%f372, %f55, %f676, %f371;
	fma.rn.f32 	%f69, %f667, %f56, %f372;
	mul.lo.s64 	%rd139, %rd29, 6364136223846793005;
	add.s64 	%rd232, %rd139, -2720673578348880933;
	// inline asm
	call(%r421,%r422,%r423,%r424,%r425,%r426,%r427,%r428,%r429,%r430,%r431,%r432,%r433,%r434,%r435,%r436,%r437,%r438,%r439,%r440,%r441,%r442,%r443,%r444,%r445,%r446,%r447,%r448,%r449,%r450,%r451,%r452),_optix_trace_typed_32,(%r339,%rd17,%f25,%f26,%f27,%f67,%f68,%f69,%f188,%f207,%f188,%r302,%r307,%r339,%r307,%r339,%r307,%r269,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339);
	// inline asm
	setp.eq.s32	%p55, %r421, -1;
	@%p55 bra 	BB0_130;

	mul.wide.u32 	%rd140, %r421, 36;
	add.s64 	%rd141, %rd18, %rd140;
	ld.global.f32 	%f373, [%rd141];
	ld.global.f32 	%f374, [%rd141+12];
	sub.f32 	%f375, %f374, %f373;
	ld.global.f32 	%f376, [%rd141+4];
	ld.global.f32 	%f377, [%rd141+16];
	sub.f32 	%f378, %f377, %f376;
	ld.global.f32 	%f379, [%rd141+8];
	ld.global.f32 	%f380, [%rd141+20];
	sub.f32 	%f381, %f380, %f379;
	ld.global.f32 	%f382, [%rd141+24];
	sub.f32 	%f383, %f382, %f373;
	ld.global.f32 	%f384, [%rd141+28];
	sub.f32 	%f385, %f384, %f376;
	ld.global.f32 	%f386, [%rd141+32];
	sub.f32 	%f387, %f386, %f379;
	sub.f32 	%f388, %f25, %f373;
	sub.f32 	%f389, %f26, %f376;
	sub.f32 	%f390, %f27, %f379;
	mul.f32 	%f391, %f378, %f387;
	mul.f32 	%f392, %f381, %f385;
	sub.f32 	%f393, %f391, %f392;
	mul.f32 	%f394, %f381, %f383;
	mul.f32 	%f395, %f375, %f387;
	sub.f32 	%f396, %f394, %f395;
	mul.f32 	%f397, %f375, %f385;
	mul.f32 	%f398, %f378, %f383;
	sub.f32 	%f399, %f397, %f398;
	mul.f32 	%f400, %f69, %f389;
	mul.f32 	%f401, %f68, %f390;
	sub.f32 	%f402, %f400, %f401;
	mul.f32 	%f403, %f67, %f390;
	mul.f32 	%f404, %f69, %f388;
	sub.f32 	%f405, %f403, %f404;
	mul.f32 	%f406, %f68, %f388;
	mul.f32 	%f407, %f67, %f389;
	sub.f32 	%f408, %f406, %f407;
	mul.f32 	%f409, %f67, %f393;
	mul.f32 	%f410, %f68, %f396;
	mul.f32 	%f411, %f69, %f399;
	add.f32 	%f412, %f411, %f410;
	add.f32 	%f413, %f409, %f412;
	rcp.rn.f32 	%f414, %f413;
	mul.f32 	%f415, %f385, %f405;
	fma.rn.f32 	%f416, %f387, %f408, %f415;
	fma.rn.f32 	%f417, %f383, %f402, %f416;
	mul.f32 	%f418, %f414, %f417;
	mul.f32 	%f419, %f378, %f405;
	fma.rn.f32 	%f420, %f381, %f408, %f419;
	fma.rn.f32 	%f421, %f375, %f402, %f420;
	mul.f32 	%f422, %f414, %f421;
	mul.f32 	%f423, %f399, %f390;
	fma.rn.f32 	%f424, %f396, %f389, %f423;
	fma.rn.f32 	%f425, %f388, %f393, %f424;
	mul.f32 	%f70, %f414, %f425;
	setp.gt.f32	%p56, %f418, 0f80000000;
	setp.lt.f32	%p57, %f418, 0fBF800000;
	or.pred  	%p58, %p56, %p57;
	setp.lt.f32	%p59, %f422, 0f00000000;
	or.pred  	%p60, %p58, %p59;
	sub.f32 	%f426, %f422, %f418;
	setp.gt.f32	%p61, %f426, 0f3F800000;
	or.pred  	%p2, %p60, %p61;
	neg.f32 	%f427, %f411;
	sub.f32 	%f428, %f427, %f410;
	sub.f32 	%f429, %f428, %f409;
	mov.b32 	 %r492, %f429;
	and.b32  	%r493, %r492, -2147483648;
	or.b32  	%r494, %r493, 1065353216;
	mov.b32 	 %f430, %r494;
	mul.f32 	%f681, %f393, %f430;
	mul.f32 	%f680, %f396, %f430;
	mul.f32 	%f679, %f399, %f430;
	mul.f32 	%f431, %f679, %f679;
	fma.rn.f32 	%f432, %f680, %f680, %f431;
	fma.rn.f32 	%f74, %f681, %f681, %f432;
	setp.leu.f32	%p62, %f74, 0f00000000;
	@%p62 bra 	BB0_54;

	sqrt.rn.f32 	%f433, %f74;
	div.rn.f32 	%f681, %f681, %f433;
	div.rn.f32 	%f680, %f680, %f433;
	div.rn.f32 	%f679, %f679, %f433;

BB0_54:
	mov.f32 	%f662, 0fBA83126F;
	mov.f32 	%f661, 0fA7C234C5;
	mov.f32 	%f660, 0fB3A22168;
	sub.f32 	%f435, %f662, %f70;
	setp.gt.f32	%p63, %f70, 0f80000000;
	or.pred  	%p64, %p2, %p63;
	selp.f32	%f436, 0f7F7FFFFF, %f435, %p64;
	max.f32 	%f438, %f188, %f436;
	fma.rn.f32 	%f81, %f67, %f438, %f25;
	fma.rn.f32 	%f82, %f68, %f438, %f26;
	fma.rn.f32 	%f83, %f69, %f438, %f27;
	mul.lo.s64 	%rd142, %rd232, 6364136223846793005;
	add.s64 	%rd41, %rd142, -2720673578348880933;
	shr.u64 	%rd143, %rd232, 18;
	xor.b64  	%rd144, %rd143, %rd232;
	shr.u64 	%rd145, %rd144, 27;
	cvt.u32.u64	%r495, %rd145;
	shr.u64 	%rd146, %rd232, 59;
	cvt.u32.u64	%r496, %rd146;
	shr.u32 	%r497, %r495, %r496;
	neg.s32 	%r498, %r496;
	and.b32  	%r499, %r498, 31;
	shl.b32 	%r500, %r495, %r499;
	or.b32  	%r501, %r497, %r500;
	shr.u32 	%r502, %r501, 9;
	or.b32  	%r503, %r502, 1065353216;
	mov.b32 	 %f439, %r503;
	add.f32 	%f440, %f439, 0fBF800000;
	shr.u64 	%rd147, %rd41, 18;
	xor.b64  	%rd148, %rd147, %rd41;
	shr.u64 	%rd149, %rd148, 27;
	cvt.u32.u64	%r504, %rd149;
	shr.u64 	%rd150, %rd41, 59;
	cvt.u32.u64	%r505, %rd150;
	shr.u32 	%r506, %r504, %r505;
	neg.s32 	%r507, %r505;
	and.b32  	%r508, %r507, 31;
	shl.b32 	%r509, %r504, %r508;
	or.b32  	%r510, %r506, %r509;
	shr.u32 	%r511, %r510, 9;
	or.b32  	%r512, %r511, 1065353216;
	mov.b32 	 %f441, %r512;
	add.f32 	%f442, %f441, 0fBF800000;
	sqrt.rn.f32 	%f84, %f440;
	mul.f32 	%f85, %f442, 0f40C90FDB;
	mul.f32 	%f443, %f85, 0f3F22F983;
	cvt.rni.s32.f32	%r858, %f443;
	cvt.rn.f32.s32	%f444, %r858;
	fma.rn.f32 	%f446, %f444, %f192, %f85;
	fma.rn.f32 	%f448, %f444, %f660, %f446;
	fma.rn.f32 	%f685, %f444, %f661, %f448;
	abs.f32 	%f87, %f85;
	setp.leu.f32	%p65, %f87, 0f47CE4780;
	mov.u32 	%r850, %r858;
	mov.f32 	%f682, %f685;
	@%p65 bra 	BB0_65;

	setp.eq.f32	%p66, %f87, 0f7F800000;
	@%p66 bra 	BB0_64;
	bra.uni 	BB0_56;

BB0_64:
	mul.rn.f32 	%f682, %f85, %f188;
	mov.u32 	%r850, %r858;
	bra.uni 	BB0_65;

BB0_56:
	mov.b32 	 %r92, %f85;
	shr.u32 	%r93, %r92, 23;
	shl.b32 	%r515, %r92, 8;
	or.b32  	%r94, %r515, -2147483648;
	mov.u32 	%r844, 0;
	mov.u64 	%rd220, __cudart_i2opi_f;
	mov.u32 	%r843, -6;
	mov.u64 	%rd221, %rd19;

BB0_57:
	.pragma "nounroll";
	ld.const.u32 	%r518, [%rd220];
	// inline asm
	{
	mad.lo.cc.u32   %r516, %r518, %r94, %r844;
	madc.hi.u32     %r844, %r518, %r94,  0;
	}
	// inline asm
	st.local.u32 	[%rd221], %r516;
	add.s64 	%rd221, %rd221, 4;
	add.s64 	%rd220, %rd220, 4;
	add.s32 	%r843, %r843, 1;
	setp.ne.s32	%p67, %r843, 0;
	@%p67 bra 	BB0_57;

	and.b32  	%r521, %r93, 255;
	add.s32 	%r522, %r521, -128;
	shr.u32 	%r523, %r522, 5;
	and.b32  	%r99, %r92, -2147483648;
	st.local.u32 	[%rd19+24], %r844;
	mov.u32 	%r524, 6;
	sub.s32 	%r525, %r524, %r523;
	mul.wide.s32 	%rd152, %r525, 4;
	add.s64 	%rd46, %rd19, %rd152;
	ld.local.u32 	%r846, [%rd46];
	ld.local.u32 	%r845, [%rd46+-4];
	and.b32  	%r102, %r93, 31;
	setp.eq.s32	%p68, %r102, 0;
	@%p68 bra 	BB0_60;

	mov.u32 	%r526, 32;
	sub.s32 	%r527, %r526, %r102;
	shr.u32 	%r528, %r845, %r527;
	shl.b32 	%r529, %r846, %r102;
	add.s32 	%r846, %r528, %r529;
	ld.local.u32 	%r530, [%rd46+-8];
	shr.u32 	%r531, %r530, %r527;
	shl.b32 	%r532, %r845, %r102;
	add.s32 	%r845, %r531, %r532;

BB0_60:
	shr.u32 	%r533, %r845, 30;
	shl.b32 	%r534, %r846, 2;
	add.s32 	%r848, %r534, %r533;
	shl.b32 	%r108, %r845, 2;
	shr.u32 	%r535, %r848, 31;
	shr.u32 	%r536, %r846, 30;
	add.s32 	%r109, %r535, %r536;
	setp.eq.s32	%p69, %r535, 0;
	@%p69 bra 	BB0_61;
	bra.uni 	BB0_62;

BB0_61:
	mov.u32 	%r847, %r108;
	mov.u32 	%r849, %r99;
	bra.uni 	BB0_63;

BB0_62:
	not.b32 	%r537, %r848;
	neg.s32 	%r847, %r108;
	setp.eq.s32	%p70, %r108, 0;
	selp.u32	%r538, 1, 0, %p70;
	add.s32 	%r848, %r538, %r537;
	xor.b32  	%r849, %r99, -2147483648;

BB0_63:
	cvt.u64.u32	%rd153, %r848;
	shl.b64 	%rd154, %rd153, 32;
	cvt.u64.u32	%rd155, %r847;
	or.b64  	%rd156, %rd154, %rd155;
	cvt.rn.f64.s64	%fd7, %rd156;
	mul.f64 	%fd8, %fd7, 0d3BF921FB54442D19;
	cvt.rn.f32.f64	%f450, %fd8;
	neg.f32 	%f451, %f450;
	setp.eq.s32	%p71, %r849, 0;
	selp.f32	%f682, %f450, %f451, %p71;
	setp.eq.s32	%p72, %r99, 0;
	neg.s32 	%r539, %r109;
	selp.b32	%r850, %r109, %r539, %p72;

BB0_65:
	add.s32 	%r118, %r850, 1;
	and.b32  	%r119, %r118, 1;
	setp.eq.s32	%p73, %r119, 0;
	selp.f32	%f91, %f682, 0f3F800000, %p73;
	mul.rn.f32 	%f92, %f682, %f682;
	fma.rn.f32 	%f93, %f92, %f91, %f188;
	mov.f32 	%f683, 0fB94D4153;
	@%p73 bra 	BB0_67;

	fma.rn.f32 	%f683, %f211, %f92, %f210;

BB0_67:
	selp.f32	%f457, 0f3C0885E4, 0f3D2AAABB, %p73;
	fma.rn.f32 	%f458, %f683, %f92, %f457;
	selp.f32	%f459, 0fBE2AAAA8, 0fBEFFFFFF, %p73;
	fma.rn.f32 	%f460, %f458, %f92, %f459;
	fma.rn.f32 	%f684, %f460, %f93, %f91;
	and.b32  	%r540, %r118, 2;
	setp.eq.s32	%p75, %r540, 0;
	@%p75 bra 	BB0_69;

	mov.f32 	%f462, 0fBF800000;
	fma.rn.f32 	%f684, %f684, %f462, %f188;

BB0_69:
	mul.f32 	%f99, %f84, %f684;
	@%p65 bra 	BB0_80;

	setp.eq.f32	%p77, %f87, 0f7F800000;
	@%p77 bra 	BB0_79;
	bra.uni 	BB0_71;

BB0_79:
	mul.rn.f32 	%f685, %f85, %f188;
	bra.uni 	BB0_80;

BB0_71:
	mov.b32 	 %r120, %f85;
	shr.u32 	%r121, %r120, 23;
	shl.b32 	%r543, %r120, 8;
	or.b32  	%r122, %r543, -2147483648;
	mov.u32 	%r852, 0;
	mov.u64 	%rd222, __cudart_i2opi_f;
	mov.u32 	%r851, -6;
	mov.u64 	%rd223, %rd19;

BB0_72:
	.pragma "nounroll";
	ld.const.u32 	%r546, [%rd222];
	// inline asm
	{
	mad.lo.cc.u32   %r544, %r546, %r122, %r852;
	madc.hi.u32     %r852, %r546, %r122,  0;
	}
	// inline asm
	st.local.u32 	[%rd223], %r544;
	add.s64 	%rd223, %rd223, 4;
	add.s64 	%rd222, %rd222, 4;
	add.s32 	%r851, %r851, 1;
	setp.ne.s32	%p78, %r851, 0;
	@%p78 bra 	BB0_72;

	and.b32  	%r549, %r121, 255;
	add.s32 	%r550, %r549, -128;
	shr.u32 	%r551, %r550, 5;
	and.b32  	%r127, %r120, -2147483648;
	st.local.u32 	[%rd19+24], %r852;
	mov.u32 	%r552, 6;
	sub.s32 	%r553, %r552, %r551;
	mul.wide.s32 	%rd158, %r553, 4;
	add.s64 	%rd51, %rd19, %rd158;
	ld.local.u32 	%r854, [%rd51];
	ld.local.u32 	%r853, [%rd51+-4];
	and.b32  	%r130, %r121, 31;
	setp.eq.s32	%p79, %r130, 0;
	@%p79 bra 	BB0_75;

	mov.u32 	%r554, 32;
	sub.s32 	%r555, %r554, %r130;
	shr.u32 	%r556, %r853, %r555;
	shl.b32 	%r557, %r854, %r130;
	add.s32 	%r854, %r556, %r557;
	ld.local.u32 	%r558, [%rd51+-8];
	shr.u32 	%r559, %r558, %r555;
	shl.b32 	%r560, %r853, %r130;
	add.s32 	%r853, %r559, %r560;

BB0_75:
	shr.u32 	%r561, %r853, 30;
	shl.b32 	%r562, %r854, 2;
	add.s32 	%r856, %r562, %r561;
	shl.b32 	%r136, %r853, 2;
	shr.u32 	%r563, %r856, 31;
	shr.u32 	%r564, %r854, 30;
	add.s32 	%r137, %r563, %r564;
	setp.eq.s32	%p80, %r563, 0;
	@%p80 bra 	BB0_76;
	bra.uni 	BB0_77;

BB0_76:
	mov.u32 	%r855, %r136;
	mov.u32 	%r857, %r127;
	bra.uni 	BB0_78;

BB0_77:
	not.b32 	%r565, %r856;
	neg.s32 	%r855, %r136;
	setp.eq.s32	%p81, %r136, 0;
	selp.u32	%r566, 1, 0, %p81;
	add.s32 	%r856, %r566, %r565;
	xor.b32  	%r857, %r127, -2147483648;

BB0_78:
	cvt.u64.u32	%rd159, %r856;
	shl.b64 	%rd160, %rd159, 32;
	cvt.u64.u32	%rd161, %r855;
	or.b64  	%rd162, %rd160, %rd161;
	cvt.rn.f64.s64	%fd9, %rd162;
	mul.f64 	%fd10, %fd9, 0d3BF921FB54442D19;
	cvt.rn.f32.f64	%f463, %fd10;
	neg.f32 	%f464, %f463;
	setp.eq.s32	%p82, %r857, 0;
	selp.f32	%f685, %f463, %f464, %p82;
	setp.eq.s32	%p83, %r127, 0;
	neg.s32 	%r567, %r137;
	selp.b32	%r858, %r137, %r567, %p83;

BB0_80:
	and.b32  	%r146, %r858, 1;
	setp.eq.s32	%p84, %r146, 0;
	selp.f32	%f103, %f685, 0f3F800000, %p84;
	mul.rn.f32 	%f104, %f685, %f685;
	fma.rn.f32 	%f105, %f104, %f103, %f188;
	mov.f32 	%f686, 0fB94D4153;
	@%p84 bra 	BB0_82;

	fma.rn.f32 	%f686, %f211, %f104, %f210;

BB0_82:
	selp.f32	%f470, 0f3C0885E4, 0f3D2AAABB, %p84;
	fma.rn.f32 	%f471, %f686, %f104, %f470;
	selp.f32	%f472, 0fBE2AAAA8, 0fBEFFFFFF, %p84;
	fma.rn.f32 	%f473, %f471, %f104, %f472;
	fma.rn.f32 	%f687, %f473, %f105, %f103;
	and.b32  	%r568, %r858, 2;
	setp.eq.s32	%p86, %r568, 0;
	@%p86 bra 	BB0_84;

	mov.f32 	%f475, 0fBF800000;
	fma.rn.f32 	%f687, %f687, %f475, %f188;

BB0_84:
	mul.f32 	%f476, %f99, %f99;
	sub.f32 	%f478, %f186, %f476;
	mul.f32 	%f111, %f84, %f687;
	mul.f32 	%f479, %f111, %f111;
	sub.f32 	%f480, %f478, %f479;
	max.f32 	%f482, %f188, %f480;
	sqrt.rn.f32 	%f112, %f482;
	abs.f32 	%f483, %f679;
	abs.f32 	%f484, %f681;
	setp.gt.f32	%p87, %f484, %f483;
	neg.f32 	%f485, %f680;
	selp.f32	%f690, %f485, 0f00000000, %p87;
	neg.f32 	%f486, %f679;
	selp.f32	%f689, %f681, %f486, %p87;
	selp.f32	%f688, 0f00000000, %f680, %p87;
	mul.f32 	%f487, %f688, %f688;
	fma.rn.f32 	%f488, %f689, %f689, %f487;
	fma.rn.f32 	%f116, %f690, %f690, %f488;
	setp.leu.f32	%p88, %f116, 0f00000000;
	@%p88 bra 	BB0_86;

	sqrt.rn.f32 	%f489, %f116;
	div.rn.f32 	%f690, %f690, %f489;
	div.rn.f32 	%f689, %f689, %f489;
	div.rn.f32 	%f688, %f688, %f489;

BB0_86:
	mul.f32 	%f499, %f680, %f688;
	mul.f32 	%f500, %f679, %f689;
	sub.f32 	%f501, %f500, %f499;
	mul.f32 	%f502, %f679, %f690;
	mul.f32 	%f503, %f681, %f688;
	sub.f32 	%f504, %f503, %f502;
	mul.f32 	%f505, %f681, %f689;
	mul.f32 	%f506, %f680, %f690;
	sub.f32 	%f507, %f506, %f505;
	mul.f32 	%f508, %f99, %f501;
	fma.rn.f32 	%f509, %f111, %f690, %f508;
	fma.rn.f32 	%f123, %f681, %f112, %f509;
	mul.f32 	%f510, %f99, %f504;
	fma.rn.f32 	%f511, %f111, %f689, %f510;
	fma.rn.f32 	%f124, %f680, %f112, %f511;
	mul.f32 	%f512, %f99, %f507;
	fma.rn.f32 	%f513, %f111, %f688, %f512;
	fma.rn.f32 	%f125, %f679, %f112, %f513;
	mul.lo.s64 	%rd164, %rd41, 6364136223846793005;
	add.s64 	%rd232, %rd164, -2720673578348880933;
	// inline asm
	call(%r569,%r570,%r571,%r572,%r573,%r574,%r575,%r576,%r577,%r578,%r579,%r580,%r581,%r582,%r583,%r584,%r585,%r586,%r587,%r588,%r589,%r590,%r591,%r592,%r593,%r594,%r595,%r596,%r597,%r598,%r599,%r600),_optix_trace_typed_32,(%r339,%rd17,%f81,%f82,%f83,%f123,%f124,%f125,%f188,%f207,%f188,%r302,%r307,%r339,%r307,%r339,%r307,%r421,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339);
	// inline asm
	setp.eq.s32	%p89, %r569, -1;
	@%p89 bra 	BB0_130;

	mul.wide.u32 	%rd165, %r569, 36;
	add.s64 	%rd166, %rd18, %rd165;
	ld.global.f32 	%f514, [%rd166];
	ld.global.f32 	%f515, [%rd166+12];
	sub.f32 	%f516, %f515, %f514;
	ld.global.f32 	%f517, [%rd166+4];
	ld.global.f32 	%f518, [%rd166+16];
	sub.f32 	%f519, %f518, %f517;
	ld.global.f32 	%f520, [%rd166+8];
	ld.global.f32 	%f521, [%rd166+20];
	sub.f32 	%f522, %f521, %f520;
	ld.global.f32 	%f523, [%rd166+24];
	sub.f32 	%f524, %f523, %f514;
	ld.global.f32 	%f525, [%rd166+28];
	sub.f32 	%f526, %f525, %f517;
	ld.global.f32 	%f527, [%rd166+32];
	sub.f32 	%f528, %f527, %f520;
	sub.f32 	%f529, %f81, %f514;
	sub.f32 	%f530, %f82, %f517;
	sub.f32 	%f531, %f83, %f520;
	mul.f32 	%f532, %f519, %f528;
	mul.f32 	%f533, %f522, %f526;
	sub.f32 	%f534, %f532, %f533;
	mul.f32 	%f535, %f522, %f524;
	mul.f32 	%f536, %f516, %f528;
	sub.f32 	%f537, %f535, %f536;
	mul.f32 	%f538, %f516, %f526;
	mul.f32 	%f539, %f519, %f524;
	sub.f32 	%f540, %f538, %f539;
	mul.f32 	%f541, %f125, %f530;
	mul.f32 	%f542, %f124, %f531;
	sub.f32 	%f543, %f541, %f542;
	mul.f32 	%f544, %f123, %f531;
	mul.f32 	%f545, %f125, %f529;
	sub.f32 	%f546, %f544, %f545;
	mul.f32 	%f547, %f124, %f529;
	mul.f32 	%f548, %f123, %f530;
	sub.f32 	%f549, %f547, %f548;
	mul.f32 	%f550, %f123, %f534;
	mul.f32 	%f551, %f124, %f537;
	mul.f32 	%f552, %f125, %f540;
	add.f32 	%f553, %f552, %f551;
	add.f32 	%f554, %f550, %f553;
	rcp.rn.f32 	%f555, %f554;
	mul.f32 	%f556, %f526, %f546;
	fma.rn.f32 	%f557, %f528, %f549, %f556;
	fma.rn.f32 	%f558, %f524, %f543, %f557;
	mul.f32 	%f559, %f555, %f558;
	mul.f32 	%f560, %f519, %f546;
	fma.rn.f32 	%f561, %f522, %f549, %f560;
	fma.rn.f32 	%f562, %f516, %f543, %f561;
	mul.f32 	%f563, %f555, %f562;
	mul.f32 	%f564, %f540, %f531;
	fma.rn.f32 	%f565, %f537, %f530, %f564;
	fma.rn.f32 	%f566, %f529, %f534, %f565;
	mul.f32 	%f126, %f555, %f566;
	setp.gt.f32	%p90, %f559, 0f80000000;
	setp.lt.f32	%p91, %f559, 0fBF800000;
	or.pred  	%p92, %p90, %p91;
	setp.lt.f32	%p93, %f563, 0f00000000;
	or.pred  	%p94, %p92, %p93;
	sub.f32 	%f567, %f563, %f559;
	setp.gt.f32	%p95, %f567, 0f3F800000;
	or.pred  	%p3, %p94, %p95;
	neg.f32 	%f568, %f552;
	sub.f32 	%f569, %f568, %f551;
	sub.f32 	%f570, %f569, %f550;
	mov.b32 	 %r640, %f570;
	and.b32  	%r641, %r640, -2147483648;
	or.b32  	%r642, %r641, 1065353216;
	mov.b32 	 %f571, %r642;
	mul.f32 	%f693, %f534, %f571;
	mul.f32 	%f692, %f537, %f571;
	mul.f32 	%f691, %f540, %f571;
	mul.f32 	%f572, %f691, %f691;
	fma.rn.f32 	%f573, %f692, %f692, %f572;
	fma.rn.f32 	%f130, %f693, %f693, %f573;
	setp.leu.f32	%p96, %f130, 0f00000000;
	@%p96 bra 	BB0_89;

	sqrt.rn.f32 	%f574, %f130;
	div.rn.f32 	%f693, %f693, %f574;
	div.rn.f32 	%f692, %f692, %f574;
	div.rn.f32 	%f691, %f691, %f574;

BB0_89:
	mov.f32 	%f665, 0fBA83126F;
	mov.f32 	%f664, 0fA7C234C5;
	mov.f32 	%f663, 0fB3A22168;
	sub.f32 	%f576, %f665, %f126;
	setp.gt.f32	%p97, %f126, 0f80000000;
	or.pred  	%p98, %p3, %p97;
	selp.f32	%f577, 0f7F7FFFFF, %f576, %p98;
	max.f32 	%f579, %f188, %f577;
	fma.rn.f32 	%f137, %f123, %f579, %f81;
	fma.rn.f32 	%f138, %f124, %f579, %f82;
	fma.rn.f32 	%f139, %f125, %f579, %f83;
	mul.lo.s64 	%rd167, %rd232, 6364136223846793005;
	add.s64 	%rd53, %rd167, -2720673578348880933;
	shr.u64 	%rd168, %rd232, 18;
	xor.b64  	%rd169, %rd168, %rd232;
	shr.u64 	%rd170, %rd169, 27;
	cvt.u32.u64	%r643, %rd170;
	shr.u64 	%rd171, %rd232, 59;
	cvt.u32.u64	%r644, %rd171;
	shr.u32 	%r645, %r643, %r644;
	neg.s32 	%r646, %r644;
	and.b32  	%r647, %r646, 31;
	shl.b32 	%r648, %r643, %r647;
	or.b32  	%r649, %r645, %r648;
	shr.u32 	%r650, %r649, 9;
	or.b32  	%r651, %r650, 1065353216;
	mov.b32 	 %f580, %r651;
	add.f32 	%f581, %f580, 0fBF800000;
	shr.u64 	%rd172, %rd53, 18;
	xor.b64  	%rd173, %rd172, %rd53;
	shr.u64 	%rd174, %rd173, 27;
	cvt.u32.u64	%r652, %rd174;
	shr.u64 	%rd175, %rd53, 59;
	cvt.u32.u64	%r653, %rd175;
	shr.u32 	%r654, %r652, %r653;
	neg.s32 	%r655, %r653;
	and.b32  	%r656, %r655, 31;
	shl.b32 	%r657, %r652, %r656;
	or.b32  	%r658, %r654, %r657;
	shr.u32 	%r659, %r658, 9;
	or.b32  	%r660, %r659, 1065353216;
	mov.b32 	 %f582, %r660;
	add.f32 	%f583, %f582, 0fBF800000;
	sqrt.rn.f32 	%f140, %f581;
	mul.f32 	%f141, %f583, 0f40C90FDB;
	mul.f32 	%f584, %f141, 0f3F22F983;
	cvt.rni.s32.f32	%r874, %f584;
	cvt.rn.f32.s32	%f585, %r874;
	fma.rn.f32 	%f587, %f585, %f192, %f141;
	fma.rn.f32 	%f589, %f585, %f663, %f587;
	fma.rn.f32 	%f697, %f585, %f664, %f589;
	abs.f32 	%f143, %f141;
	setp.leu.f32	%p99, %f143, 0f47CE4780;
	mov.u32 	%r866, %r874;
	mov.f32 	%f694, %f697;
	@%p99 bra 	BB0_100;

	setp.eq.f32	%p100, %f143, 0f7F800000;
	@%p100 bra 	BB0_99;
	bra.uni 	BB0_91;

BB0_99:
	mul.rn.f32 	%f694, %f141, %f188;
	mov.u32 	%r866, %r874;
	bra.uni 	BB0_100;

BB0_91:
	mov.b32 	 %r149, %f141;
	shr.u32 	%r150, %r149, 23;
	shl.b32 	%r663, %r149, 8;
	or.b32  	%r151, %r663, -2147483648;
	mov.u32 	%r860, 0;
	mov.u64 	%rd224, __cudart_i2opi_f;
	mov.u32 	%r859, -6;
	mov.u64 	%rd225, %rd19;

BB0_92:
	.pragma "nounroll";
	ld.const.u32 	%r666, [%rd224];
	// inline asm
	{
	mad.lo.cc.u32   %r664, %r666, %r151, %r860;
	madc.hi.u32     %r860, %r666, %r151,  0;
	}
	// inline asm
	st.local.u32 	[%rd225], %r664;
	add.s64 	%rd225, %rd225, 4;
	add.s64 	%rd224, %rd224, 4;
	add.s32 	%r859, %r859, 1;
	setp.ne.s32	%p101, %r859, 0;
	@%p101 bra 	BB0_92;

	and.b32  	%r669, %r150, 255;
	add.s32 	%r670, %r669, -128;
	shr.u32 	%r671, %r670, 5;
	and.b32  	%r156, %r149, -2147483648;
	st.local.u32 	[%rd19+24], %r860;
	mov.u32 	%r672, 6;
	sub.s32 	%r673, %r672, %r671;
	mul.wide.s32 	%rd177, %r673, 4;
	add.s64 	%rd58, %rd19, %rd177;
	ld.local.u32 	%r862, [%rd58];
	ld.local.u32 	%r861, [%rd58+-4];
	and.b32  	%r159, %r150, 31;
	setp.eq.s32	%p102, %r159, 0;
	@%p102 bra 	BB0_95;

	mov.u32 	%r674, 32;
	sub.s32 	%r675, %r674, %r159;
	shr.u32 	%r676, %r861, %r675;
	shl.b32 	%r677, %r862, %r159;
	add.s32 	%r862, %r676, %r677;
	ld.local.u32 	%r678, [%rd58+-8];
	shr.u32 	%r679, %r678, %r675;
	shl.b32 	%r680, %r861, %r159;
	add.s32 	%r861, %r679, %r680;

BB0_95:
	shr.u32 	%r681, %r861, 30;
	shl.b32 	%r682, %r862, 2;
	add.s32 	%r864, %r682, %r681;
	shl.b32 	%r165, %r861, 2;
	shr.u32 	%r683, %r864, 31;
	shr.u32 	%r684, %r862, 30;
	add.s32 	%r166, %r683, %r684;
	setp.eq.s32	%p103, %r683, 0;
	@%p103 bra 	BB0_96;
	bra.uni 	BB0_97;

BB0_96:
	mov.u32 	%r863, %r165;
	mov.u32 	%r865, %r156;
	bra.uni 	BB0_98;

BB0_97:
	not.b32 	%r685, %r864;
	neg.s32 	%r863, %r165;
	setp.eq.s32	%p104, %r165, 0;
	selp.u32	%r686, 1, 0, %p104;
	add.s32 	%r864, %r686, %r685;
	xor.b32  	%r865, %r156, -2147483648;

BB0_98:
	cvt.u64.u32	%rd178, %r864;
	shl.b64 	%rd179, %rd178, 32;
	cvt.u64.u32	%rd180, %r863;
	or.b64  	%rd181, %rd179, %rd180;
	cvt.rn.f64.s64	%fd11, %rd181;
	mul.f64 	%fd12, %fd11, 0d3BF921FB54442D19;
	cvt.rn.f32.f64	%f591, %fd12;
	neg.f32 	%f592, %f591;
	setp.eq.s32	%p105, %r865, 0;
	selp.f32	%f694, %f591, %f592, %p105;
	setp.eq.s32	%p106, %r156, 0;
	neg.s32 	%r687, %r166;
	selp.b32	%r866, %r166, %r687, %p106;

BB0_100:
	add.s32 	%r175, %r866, 1;
	and.b32  	%r176, %r175, 1;
	setp.eq.s32	%p107, %r176, 0;
	selp.f32	%f147, %f694, 0f3F800000, %p107;
	mul.rn.f32 	%f148, %f694, %f694;
	fma.rn.f32 	%f149, %f148, %f147, %f188;
	mov.f32 	%f695, 0fB94D4153;
	@%p107 bra 	BB0_102;

	fma.rn.f32 	%f695, %f211, %f148, %f210;

BB0_102:
	selp.f32	%f598, 0f3C0885E4, 0f3D2AAABB, %p107;
	fma.rn.f32 	%f599, %f695, %f148, %f598;
	selp.f32	%f600, 0fBE2AAAA8, 0fBEFFFFFF, %p107;
	fma.rn.f32 	%f601, %f599, %f148, %f600;
	fma.rn.f32 	%f696, %f601, %f149, %f147;
	and.b32  	%r688, %r175, 2;
	setp.eq.s32	%p109, %r688, 0;
	@%p109 bra 	BB0_104;

	mov.f32 	%f603, 0fBF800000;
	fma.rn.f32 	%f696, %f696, %f603, %f188;

BB0_104:
	mul.f32 	%f155, %f140, %f696;
	@%p99 bra 	BB0_115;

	setp.eq.f32	%p111, %f143, 0f7F800000;
	@%p111 bra 	BB0_114;
	bra.uni 	BB0_106;

BB0_114:
	mul.rn.f32 	%f697, %f141, %f188;
	bra.uni 	BB0_115;

BB0_106:
	mov.b32 	 %r177, %f141;
	shr.u32 	%r178, %r177, 23;
	shl.b32 	%r691, %r177, 8;
	or.b32  	%r179, %r691, -2147483648;
	mov.u32 	%r868, 0;
	mov.u64 	%rd226, __cudart_i2opi_f;
	mov.u32 	%r867, -6;
	mov.u64 	%rd227, %rd19;

BB0_107:
	.pragma "nounroll";
	ld.const.u32 	%r694, [%rd226];
	// inline asm
	{
	mad.lo.cc.u32   %r692, %r694, %r179, %r868;
	madc.hi.u32     %r868, %r694, %r179,  0;
	}
	// inline asm
	st.local.u32 	[%rd227], %r692;
	add.s64 	%rd227, %rd227, 4;
	add.s64 	%rd226, %rd226, 4;
	add.s32 	%r867, %r867, 1;
	setp.ne.s32	%p112, %r867, 0;
	@%p112 bra 	BB0_107;

	and.b32  	%r697, %r178, 255;
	add.s32 	%r698, %r697, -128;
	shr.u32 	%r699, %r698, 5;
	and.b32  	%r184, %r177, -2147483648;
	st.local.u32 	[%rd19+24], %r868;
	mov.u32 	%r700, 6;
	sub.s32 	%r701, %r700, %r699;
	mul.wide.s32 	%rd183, %r701, 4;
	add.s64 	%rd63, %rd19, %rd183;
	ld.local.u32 	%r870, [%rd63];
	ld.local.u32 	%r869, [%rd63+-4];
	and.b32  	%r187, %r178, 31;
	setp.eq.s32	%p113, %r187, 0;
	@%p113 bra 	BB0_110;

	mov.u32 	%r702, 32;
	sub.s32 	%r703, %r702, %r187;
	shr.u32 	%r704, %r869, %r703;
	shl.b32 	%r705, %r870, %r187;
	add.s32 	%r870, %r704, %r705;
	ld.local.u32 	%r706, [%rd63+-8];
	shr.u32 	%r707, %r706, %r703;
	shl.b32 	%r708, %r869, %r187;
	add.s32 	%r869, %r707, %r708;

BB0_110:
	shr.u32 	%r709, %r869, 30;
	shl.b32 	%r710, %r870, 2;
	add.s32 	%r872, %r710, %r709;
	shl.b32 	%r193, %r869, 2;
	shr.u32 	%r711, %r872, 31;
	shr.u32 	%r712, %r870, 30;
	add.s32 	%r194, %r711, %r712;
	setp.eq.s32	%p114, %r711, 0;
	@%p114 bra 	BB0_111;
	bra.uni 	BB0_112;

BB0_111:
	mov.u32 	%r871, %r193;
	mov.u32 	%r873, %r184;
	bra.uni 	BB0_113;

BB0_112:
	not.b32 	%r713, %r872;
	neg.s32 	%r871, %r193;
	setp.eq.s32	%p115, %r193, 0;
	selp.u32	%r714, 1, 0, %p115;
	add.s32 	%r872, %r714, %r713;
	xor.b32  	%r873, %r184, -2147483648;

BB0_113:
	cvt.u64.u32	%rd184, %r872;
	shl.b64 	%rd185, %rd184, 32;
	cvt.u64.u32	%rd186, %r871;
	or.b64  	%rd187, %rd185, %rd186;
	cvt.rn.f64.s64	%fd13, %rd187;
	mul.f64 	%fd14, %fd13, 0d3BF921FB54442D19;
	cvt.rn.f32.f64	%f604, %fd14;
	neg.f32 	%f605, %f604;
	setp.eq.s32	%p116, %r873, 0;
	selp.f32	%f697, %f604, %f605, %p116;
	setp.eq.s32	%p117, %r184, 0;
	neg.s32 	%r715, %r194;
	selp.b32	%r874, %r194, %r715, %p117;

BB0_115:
	and.b32  	%r203, %r874, 1;
	setp.eq.s32	%p118, %r203, 0;
	selp.f32	%f159, %f697, 0f3F800000, %p118;
	mul.rn.f32 	%f160, %f697, %f697;
	fma.rn.f32 	%f161, %f160, %f159, %f188;
	mov.f32 	%f698, 0fB94D4153;
	@%p118 bra 	BB0_117;

	fma.rn.f32 	%f698, %f211, %f160, %f210;

BB0_117:
	selp.f32	%f611, 0f3C0885E4, 0f3D2AAABB, %p118;
	fma.rn.f32 	%f612, %f698, %f160, %f611;
	selp.f32	%f613, 0fBE2AAAA8, 0fBEFFFFFF, %p118;
	fma.rn.f32 	%f614, %f612, %f160, %f613;
	fma.rn.f32 	%f699, %f614, %f161, %f159;
	and.b32  	%r716, %r874, 2;
	setp.eq.s32	%p120, %r716, 0;
	@%p120 bra 	BB0_119;

	mov.f32 	%f616, 0fBF800000;
	fma.rn.f32 	%f699, %f699, %f616, %f188;

BB0_119:
	mul.f32 	%f617, %f155, %f155;
	sub.f32 	%f619, %f186, %f617;
	mul.f32 	%f167, %f140, %f699;
	mul.f32 	%f620, %f167, %f167;
	sub.f32 	%f621, %f619, %f620;
	max.f32 	%f623, %f188, %f621;
	sqrt.rn.f32 	%f168, %f623;
	abs.f32 	%f624, %f691;
	abs.f32 	%f625, %f693;
	setp.gt.f32	%p121, %f625, %f624;
	neg.f32 	%f626, %f692;
	selp.f32	%f702, %f626, 0f00000000, %p121;
	neg.f32 	%f627, %f691;
	selp.f32	%f701, %f693, %f627, %p121;
	selp.f32	%f700, 0f00000000, %f692, %p121;
	mul.f32 	%f628, %f700, %f700;
	fma.rn.f32 	%f629, %f701, %f701, %f628;
	fma.rn.f32 	%f172, %f702, %f702, %f629;
	setp.leu.f32	%p122, %f172, 0f00000000;
	@%p122 bra 	BB0_121;

	sqrt.rn.f32 	%f630, %f172;
	div.rn.f32 	%f702, %f702, %f630;
	div.rn.f32 	%f701, %f701, %f630;
	div.rn.f32 	%f700, %f700, %f630;

BB0_121:
	mul.f32 	%f640, %f692, %f700;
	mul.f32 	%f641, %f691, %f701;
	sub.f32 	%f642, %f641, %f640;
	mul.f32 	%f643, %f691, %f702;
	mul.f32 	%f644, %f693, %f700;
	sub.f32 	%f645, %f644, %f643;
	mul.f32 	%f646, %f693, %f701;
	mul.f32 	%f647, %f692, %f702;
	sub.f32 	%f648, %f647, %f646;
	mul.f32 	%f649, %f155, %f642;
	fma.rn.f32 	%f650, %f167, %f702, %f649;
	fma.rn.f32 	%f634, %f693, %f168, %f650;
	mul.f32 	%f651, %f155, %f645;
	fma.rn.f32 	%f652, %f167, %f701, %f651;
	fma.rn.f32 	%f635, %f692, %f168, %f652;
	mul.f32 	%f653, %f155, %f648;
	fma.rn.f32 	%f654, %f167, %f700, %f653;
	fma.rn.f32 	%f636, %f691, %f168, %f654;
	mul.lo.s64 	%rd189, %rd53, 6364136223846793005;
	add.s64 	%rd232, %rd189, -2720673578348880933;
	// inline asm
	call(%r879,%r718,%r719,%r720,%r721,%r722,%r723,%r724,%r725,%r726,%r727,%r728,%r729,%r730,%r731,%r732,%r733,%r734,%r735,%r736,%r737,%r738,%r739,%r740,%r741,%r742,%r743,%r744,%r745,%r746,%r747,%r748),_optix_trace_typed_32,(%r339,%rd17,%f137,%f138,%f139,%f634,%f635,%f636,%f188,%f207,%f188,%r302,%r307,%r339,%r307,%r339,%r307,%r569,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339,%r339);
	// inline asm
	setp.eq.s32	%p123, %r879, -1;
	@%p123 bra 	BB0_130;
	bra.uni 	BB0_122;

BB0_130:
	add.s32 	%r817, %r817, 1;
	setp.gt.u32	%p128, %r817, 2;
	mov.u32 	%r879, -1;
	@%p128 bra 	BB0_133;

BB0_131:
	add.s32 	%r816, %r816, 1;
	setp.lt.u32	%p129, %r816, 32;
	@%p129 bra 	BB0_5;

	ld.const.u64 	%rd202, [params+16];
	cvta.to.global.u64 	%rd203, %rd202;
	shl.b64 	%rd204, %rd1, 2;
	add.s64 	%rd205, %rd203, %rd204;
	ld.global.f32 	%f658, [%rd205];
	neg.f32 	%f659, %f658;
	st.global.f32 	[%rd205], %f659;

BB0_133:
	ret;
}

	// .globl	__miss__ms
.visible .entry __miss__ms(

)
{
	.reg .b32 	%r<3>;


	mov.u32 	%r1, 0;
	mov.u32 	%r2, -1;
	// inline asm
	call _optix_set_payload, (%r1, %r2);
	// inline asm
	ret;
}

	// .globl	__closesthit__ch
.visible .entry __closesthit__ch(

)
{
	.reg .b32 	%r<4>;


	// inline asm
	call (%r1), _optix_read_primitive_idx, ();
	// inline asm
	mov.u32 	%r2, 0;
	// inline asm
	call _optix_set_payload, (%r2, %r1);
	// inline asm
	ret;
}


